{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd481f25-b346-4ebb-b28a-cec1507e6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# код из ДЗ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86b8de7-53aa-447e-8863-0bd4dc52e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "\n",
    "def preprocess_inplace(data: pd.DataFrame):\n",
    "    data['date_time'] = pd.to_datetime(data['date_time'])\n",
    "    data.sort_values(by='date_time', inplace=True)\n",
    "    \n",
    "\n",
    "from typing import List, Tuple, Union, Optional, Dict\n",
    "\n",
    "Feature = Tuple[List[str], np.array]\n",
    "CategoricalFeature = Tuple[List[str], csc_matrix]\n",
    "\n",
    "def categorical_feature_from_series(series: pd.Series) -> CategoricalFeature:\n",
    "    dummies = pd.get_dummies(series, prefix=series.name, sparse=True)\n",
    "    return dummies.columns.tolist(), csc_matrix(dummies.sparse.to_coo())        \n",
    "\n",
    "def product_of_categorical_features(\n",
    "    feature1: CategoricalFeature, feature2: CategoricalFeature\n",
    ") -> CategoricalFeature:\n",
    "    names1, feature1 = feature1\n",
    "    names2, feature2 = feature2\n",
    "    \n",
    "    new_names = []\n",
    "    new_columns = []\n",
    "    for i, name1 in enumerate(names1):\n",
    "        for j, name2 in enumerate(names2):\n",
    "            new_names.append(f\"{name1}__{name2}\")\n",
    "            new_columns.append(feature1[:, i].multiply(feature2[:, j]))\n",
    "    \n",
    "    return new_names, scipy.sparse.hstack(new_columns)\n",
    "\n",
    "def feature_engineering(data: pd.DataFrame, feature_products: List[Tuple[str, str]] = []) -> Feature:\n",
    "    cat_feature_names = [\"zone_id\", \"banner_id\", \"os_id\", \"country_id\"]\n",
    "    features = [categorical_feature_from_series(data[name]) for name in cat_feature_names]\n",
    "    \n",
    "    cat_feature_dict = dict(zip(cat_feature_names, features))\n",
    "    for name1, name2 in feature_products:\n",
    "        product_feature = product_of_categorical_features(cat_feature_dict[name1], cat_feature_dict[name2])\n",
    "        features.append(product_feature)\n",
    "    \n",
    "    # features.append(([\"log_clicks\"], csc_matrix(np.log(1 + data.campaign_clicks.values.reshape(-1, 1)))))\n",
    "    \n",
    "    # gather everything\n",
    "    all_names, all_features = [], []\n",
    "    for names, csc in features:\n",
    "        all_names.extend(names)\n",
    "        all_features.append(csc)\n",
    "    all_features = csr_matrix(scipy.sparse.hstack(all_features))\n",
    "    \n",
    "    return all_names, all_features\n",
    "\n",
    "# %time basic_features = feature_engineering(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fea4ac0-152f-43ec-b7c2-17dc54f14da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_day_eval_split(data, X, y):\n",
    "    last_event = data.date_time.iloc[-1]\n",
    "    day = last_event.day\n",
    "    month = last_event.month\n",
    "    year = last_event.year\n",
    "    events_on_last_day = ((data.date_time.dt.day == day) & (data.date_time.dt.month == month) & (data.date_time.dt.year == year)).sum()\n",
    "    \n",
    "    X_train = X[:-events_on_last_day]\n",
    "    y_train = y[:-events_on_last_day]\n",
    "    \n",
    "    X_eval = X[-events_on_last_day:]\n",
    "    y_eval = y[-events_on_last_day:]\n",
    "    \n",
    "    data_train = data.iloc[:-events_on_last_day]\n",
    "    data_eval = data.iloc[-events_on_last_day:]\n",
    "    \n",
    "    return data_train, data_eval, X_train, y_train, X_eval, y_eval\n",
    "    \n",
    "def train_val_split(data, X, y):\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    val = n // 10\n",
    "    \n",
    "    X_train = X[:-val]\n",
    "    X_val = X[-val:]\n",
    "    \n",
    "    y_train = y[:-val]\n",
    "    y_val = y[-val:]\n",
    "    \n",
    "    data_train = data.iloc[:-val]\n",
    "    data_val = data.iloc[-val:]\n",
    "    \n",
    "    return data_train, data_val, X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882446c9-ebee-4cba-b4d7-218c8e400ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def create_model(X, y, C=1.0):\n",
    "    y = y.astype(np.float64).todense().reshape((-1, 1))  # no idea why\n",
    "    model = LogisticRegression(penalty='l2', C=C, solver='lbfgs')\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39201e47-16ac-4803-8b45-e0e2d8fd0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# фильтруем чтобы суммы в уравнениях имели смысл (не можем применить модель к banner_id0 и banner_id1 которых не было на обучении)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e175c3-170e-40d3-9ddc-2d442bef37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_and_features_for_cips(data, features):\n",
    "    bannerid_equals_bannerid0 = (data['banner_id'] == data['banner_id0'])\n",
    "    banners_in_train_set = set(data['banner_id'])\n",
    "    have_coefficients_for_banners_in_train_set = data['banner_id1'].map(lambda i: i in banners_in_train_set)\n",
    "    \n",
    "    data_for_cips = data[bannerid_equals_bannerid0 & have_coefficients_for_banners_in_train_set].copy()\n",
    "    features_for_cips = features[bannerid_equals_bannerid0 & have_coefficients_for_banners_in_train_set].copy()\n",
    "    \n",
    "    return data_for_cips, features_for_cips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4181c1d-cc8f-49a5-9337-5fd66d17989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %data_for_cips, features_for_cips = filter_dataframe_and_features_for_cips(data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39e2ae02-4f7f-4bd3-a518-a8c6137e7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit\n",
    "from scipy.sparse import lil_matrix\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def bannerid_to_bannerid1(data: pd.DataFrame, features: csc_matrix, feature_names: List[str]) -> lil_matrix:\n",
    "    banner_id_to_feature_id = {}\n",
    "    for idx, name in enumerate(feature_names):\n",
    "        if name.startswith(\"banner_id\"):\n",
    "            _, banner_id = name.rsplit(\"_\", maxsplit=1)\n",
    "            banner_id_to_feature_id[int(banner_id)] = idx\n",
    "    \n",
    "    n, _ = features.shape\n",
    "    \n",
    "    banner_pairs: List[Dict[str, int]] = data[['banner_id', 'banner_id1']].to_dict('records')\n",
    "    features_bid1: lil_matrix = features.tolil()\n",
    "    for row_idx, row in enumerate(banner_pairs):\n",
    "        to_unset = banner_id_to_feature_id[row['banner_id']]\n",
    "        to_set = banner_id_to_feature_id[row['banner_id1']]\n",
    "        features_bid1[row_idx, to_unset] = 0.0\n",
    "        features_bid1[row_idx, to_set] = 1.0\n",
    "        \n",
    "    return features_bid1\n",
    "\n",
    "\n",
    "def normal1_greater_than_normal2(mu1, sigma1, mu2, sigma2):\n",
    "    mu_diff = mu1 - mu2\n",
    "    sigma_diff = np.sqrt(sigma1 ** 2 + sigma2 ** 2)\n",
    "    return 1 - norm.cdf(0, loc=mu_diff, scale=sigma_diff)\n",
    "\n",
    "\n",
    "def cips(\n",
    "    data: pd.DataFrame,\n",
    "    features_bid0: csc_matrix,\n",
    "    features_bid1: lil_matrix,\n",
    "    model: LogisticRegression,\n",
    "    lambda_: float = 10.\n",
    "):\n",
    "    # по условию задания\n",
    "    coeff_sum0_new = logit(model.predict_proba(features_bid0)[:, -1])\n",
    "    coeff_sum1_new = logit(model.predict_proba(features_bid1)[:, -1])\n",
    "    \n",
    "    # \"старая\" policy\n",
    "    pi0 = normal1_greater_than_normal2(\n",
    "        mu1=data.coeff_sum0,\n",
    "        sigma1=data.g0,\n",
    "        mu2=data.coeff_sum1,\n",
    "        sigma2=data.g1\n",
    "    )\n",
    "    \n",
    "    # новая policy\n",
    "    pi1 = normal1_greater_than_normal2(\n",
    "        mu1=coeff_sum0_new,\n",
    "        sigma1=data.g0,\n",
    "        mu2=coeff_sum1_new,\n",
    "        sigma2=data.g1\n",
    "    )\n",
    "    \n",
    "    reward = data.clicks.values\n",
    "    clipped_ips = np.nanmean(\n",
    "        np.minimum(pi1 / pi0, lambda_) * reward\n",
    "    )\n",
    "    \n",
    "    return clipped_ips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57daea6a-0264-458c-8812-f8837130db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"большая\" функция которая делает всё\n",
    "def calculate_cips(data, features, feature_names, model):\n",
    "    data_for_cips, features_for_cips = filter_dataframe_and_features_for_cips(data, features)\n",
    "    features_for_cips_bid1 = bannerid_to_bannerid1(data_for_cips, features_for_cips, feature_names)\n",
    "    return cips(\n",
    "        data=data_for_cips,\n",
    "        features_bid0=features_for_cips,\n",
    "        features_bid1=features_for_cips_bid1,\n",
    "        model=model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71a36f-fc98-495f-a71a-888120368676",
   "metadata": {},
   "source": [
    "### Подбираем C опираясь на Clipped IPS в качестве метрики (в первом дз использовался log loss на последнем дне)\n",
    "\n",
    "Off-Policy Learning: $\\hat{\\pi} = \\arg \\max_{\\pi \\in \\Pi} \\hat{V}_{CIPS}(\\pi;D_0, \\lambda=10)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f16569e4-942e-4233-8a8e-5798391edfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(data, features, feature_names, answers):\n",
    "    models = {}\n",
    "    \n",
    "    val_cipses = {}\n",
    "    \n",
    "    C_grid = [0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]  # larger do not converge\n",
    "    data_, data_eval, X, y, X_eval, y_eval = last_day_eval_split(data, features, answers)\n",
    "    data_train, data_val, X_train, y_train, X_val, y_val = train_val_split(data_, X, y)\n",
    "\n",
    "    for C in C_grid:\n",
    "        model = create_model(X_train, y_train, C)\n",
    "        \n",
    "        val_loss = log_loss(y_val.todense(), model.predict_proba(X_val))\n",
    "        val_cips = calculate_cips(data_val, X_val, feature_names, model)\n",
    "        \n",
    "        models[C] = model\n",
    "        val_cipses[C] = val_cips \n",
    "        \n",
    "        print(f\"C={C:.6f}\\tval_logloss={val_loss:.6f}\\tval_CIPS={val_cips:.6f}\")\n",
    "        \n",
    "    best_cips = -1\n",
    "    best_model = None\n",
    "    best_model_C = None\n",
    "    for C, cips_ in val_cipses.items():\n",
    "        if cips_ > best_cips:\n",
    "            best_cips = cips_\n",
    "            best_model = models[C]\n",
    "            best_C = C\n",
    "    \n",
    "    lastday_logloss = log_loss(y_eval.todense(), best_model.predict_proba(X_eval))\n",
    "    lastday_cips = eval_cips = calculate_cips(data_eval, X_eval, feature_names, best_model)\n",
    "    \n",
    "    return {\n",
    "        \"model\": best_model,\n",
    "        \"lastday_cips\": lastday_cips,\n",
    "        \"lastday_logloss\": lastday_logloss,\n",
    "        \"C\": best_C\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414076c3-31fd-45e1-aef3-1ad0315b8c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 832 ms, total: 16.3 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/data.csv')\n",
    "preprocess_inplace(data)\n",
    "\n",
    "answers = csr_matrix(data.clicks.values.reshape(-1, 1))\n",
    "%time names, features = feature_engineering(data, feature_products=[[\"os_id\", \"country_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6e5739e-2655-4950-9a88-7310f2982d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.000001\tval_logloss=0.185062\tval_CIPS=0.065711\n",
      "C=0.000005\tval_logloss=0.182732\tval_CIPS=0.066546\n",
      "C=0.000010\tval_logloss=0.180679\tval_CIPS=0.066604\n",
      "C=0.000050\tval_logloss=0.173228\tval_CIPS=0.062075\n",
      "C=0.000100\tval_logloss=0.169614\tval_CIPS=0.059645\n",
      "C=0.000500\tval_logloss=0.163407\tval_CIPS=0.059128\n",
      "C=0.001000\tval_logloss=0.161664\tval_CIPS=0.058043\n",
      "C=0.005000\tval_logloss=0.159087\tval_CIPS=0.059951\n",
      "C=0.010000\tval_logloss=0.158519\tval_CIPS=0.058464\n",
      "{'model': LogisticRegression(C=1e-05), 'lastday_cips': 0.045828518319138006, 'lastday_logloss': 0.15187974190966094, 'C': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    warnings.warn(\"\", RuntimeWarning)\n",
    "    warnings.warn(\"\", FutureWarning)\n",
    "    warnings.warn(\"\", DataConversionWarning)\n",
    "    \n",
    "    cv_result = cv(data, features, names, answers)\n",
    "    \n",
    "    print(cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e5789-d250-4e35-a3f8-5e1d80381362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
