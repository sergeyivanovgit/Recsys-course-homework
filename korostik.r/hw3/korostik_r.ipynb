{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf81d9e-fed8-4684-8535-dfe661f882bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xlearn as xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf97ec5f-9e1a-44fa-a4c8-91852bbee229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим данные для xlearn\n",
    "# разбиваем на \"последний день\" и \"остальное\"\n",
    "# разбиваем \"остальное\" на train (90%), val1 (5%) и val2 (5%)\n",
    "\n",
    "# все разбиения по времени\n",
    "# то есть train раньше val1 раньше val2, они все раньше последнего для\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def last_day_eval_split(data):\n",
    "    last_event = data.date_time.iloc[-1]\n",
    "    day = last_event.day\n",
    "    month = last_event.month\n",
    "    year = last_event.year\n",
    "    k = ((data.date_time.dt.day == day) & (data.date_time.dt.month == month) & (data.date_time.dt.year == year)).sum()\n",
    "    \n",
    "    data = data.iloc[:-k]\n",
    "    lastday = data.iloc[-k:]\n",
    "    \n",
    "    return data, lastday\n",
    "    \n",
    "    \n",
    "def train_val_split(data):\n",
    "    n = len(data)\n",
    "    \n",
    "    # 5% for both validation sets\n",
    "    k = n // 20\n",
    "    \n",
    "    train = data.iloc[:-(k * 2)]\n",
    "    val1 = data.iloc[-(k * 2):-k]\n",
    "    val2 = data.iloc[-k:]\n",
    "    \n",
    "    return train, val1, val2\n",
    "\n",
    "\n",
    "def to_libffm(dataframe):\n",
    "    for _, row in dataframe.iterrows():\n",
    "        yield f\"{row.clicks} 0:{row.banner_id}:1 1:{row.zone_id}:1 2:{row.country_id}:1 3:{row.os_id}:1 4:{row.oaid_trimmed}:1\"\n",
    "\n",
    "\n",
    "def prepare_dataframe(dataframe, path: Path):\n",
    "    # shuffle the dataframe just in case\n",
    "    dataframe = dataframe.sample(frac = 1)\n",
    "    with path.open(\"w\") as f:\n",
    "        for line in tqdm(to_libffm(dataframe), total=len(dataframe)):\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "        \n",
    "def preprocess():\n",
    "    data = pd.read_csv('../data/data.csv')\n",
    "    data['date_time'] = pd.to_datetime(data['date_time'])\n",
    "    data.sort_values(by='date_time', inplace=True)\n",
    "    \n",
    "    # shift feature numbers so they wouldn't intersect\n",
    "    data.zone_id = data.zone_id + data.banner_id.max() + 1\n",
    "    data.country_id = data.country_id + data.zone_id.max() + 1\n",
    "    data.os_id = data.os_id + data.country_id.max() + 1\n",
    "    \n",
    "    def trim_user_id(user_id):\n",
    "        # we have over 6M different userid hashes\n",
    "        # so let's take top 0.1% for additional 6k features\n",
    "        counts = user_id.value_counts()\n",
    "        lower_bound = np.percentile(counts, 99.9)\n",
    "        \n",
    "        userid_map = {}\n",
    "        for userid, count in counts.iteritems():\n",
    "            if count > lower_bound:\n",
    "                userid_map.setdefault(userid, len(userid_map))\n",
    "        \n",
    "        return user_id.apply(lambda x: userid_map.get(x, len(userid_map)))\n",
    "    \n",
    "    data[\"oaid_trimmed\"] = trim_user_id(data.oaid_hash) + data.os_id.max() + 1\n",
    "    \n",
    "    data, lastday = last_day_eval_split(data)\n",
    "    train, val1, val2 = train_val_split(data)\n",
    "    \n",
    "    prepare_dataframe(train, Path(\"../data/train.txt\"))\n",
    "    prepare_dataframe(val1, Path(\"../data/val1.txt\"))\n",
    "    prepare_dataframe(val2, Path(\"../data/val2.txt\"))\n",
    "    prepare_dataframe(lastday, Path(\"../data/lastday.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46942152-a199-4c0b-98bb-bf66b7aa7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# занимает около 13 мин на моем пк\n",
    "# preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c544895c-294d-42fd-bbf9-a88013f3921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def validate(model_path, test_set=\"../data/val2.txt\") -> float:\n",
    "    ffm_model = xl.create_ffm()\n",
    "    ffm_model.setSigmoid()\n",
    "    ffm_model.setTest(test_set)\n",
    "    ffm_model.predict(model_path, \"./output.txt\")\n",
    "    \n",
    "    y_pred = []\n",
    "    with open(\"./output.txt\") as f:\n",
    "        for line in f:\n",
    "            prob = float(line.strip())\n",
    "            y_pred.append(prob)\n",
    "            \n",
    "    y_true = []\n",
    "    with open(test_set) as f:\n",
    "        for line in f:\n",
    "            label, _ = line.split(maxsplit=1)\n",
    "            y_true.append(float(label))\n",
    "    \n",
    "    return log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73d0c117-ac65-46d4-be89-beedfd190f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подбираем параметры при помощи bayesian optimization\n",
    "# k от 4 до 32\n",
    "# lr от 0.2 до 0.005\n",
    "# lambda от 0.00002 до 0.002\n",
    "# все три параметра в логарифмической шкале\n",
    "\n",
    "# предлагаем три \"стартовые\" точки:\n",
    "# k=4 lr=0.2 lambda=0.00002\n",
    "# k=8 lr=0.2 lambda=0.00002\n",
    "# k=8 lr=0.2 lambda=0.0002\n",
    "\n",
    "# максимум 100 эпох\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.bayesopt = None\n",
    "        self.models = []\n",
    "        self.params = []\n",
    "        \n",
    "        self.valset = \"../data/val1.txt\"\n",
    "        \n",
    "    def start(self, n_iter):\n",
    "        pbounds = {\n",
    "            'k_': [np.log2(4), np.log2(32)],\n",
    "            'lr_': [np.log10(0.005), np.log10(0.02)],\n",
    "            'l2_coeff_': [np.log10(0.00002), np.log10(0.002)]\n",
    "        }\n",
    "        self.bayesopt = BayesianOptimization(f=self.blackbox, pbounds=pbounds)\n",
    "        self.bayesopt.probe(\n",
    "            params={\"k_\": np.log2(8), \"lr_\": np.log10(0.2), \"l2_coeff_\": np.log10(0.00002)},\n",
    "            lazy=True,\n",
    "        )\n",
    "        self.bayesopt.probe(\n",
    "            params={\"k_\": np.log2(8), \"lr_\": np.log10(0.2), \"l2_coeff_\": np.log10(0.0002)},\n",
    "            lazy=True,\n",
    "        )\n",
    "        self.bayesopt.probe(\n",
    "            params={\"k_\": np.log2(4), \"lr_\": np.log10(0.2), \"l2_coeff_\": np.log10(0.00002)},\n",
    "            lazy=True,\n",
    "        )\n",
    "        \n",
    "        self.bayesopt.maximize(\n",
    "            init_points=5,\n",
    "            n_iter=n_iter,\n",
    "        )                        \n",
    "                                             \n",
    "    def blackbox(self, k_, lr_, l2_coeff_):\n",
    "        return self.train(int(2 ** k_), 10 ** lr_, 10 ** l2_coeff_)\n",
    "    \n",
    "    def train(self, k, lr, l2_coeff) -> float:\n",
    "        ffm_model = xl.create_ffm()\n",
    "        ffm_model.setTrain(\"../data/train.txt\")\n",
    "        ffm_model.setValidate(\"../data/val1.txt\")\n",
    "\n",
    "        param = {\"task\": \"binary\", \"k\": k, \"lr\": lr, \"epoch\": 100, \"lambda\": l2_coeff}\n",
    "        model_path = f\"./model_k={k}_lr={lr}_lambda={l2_coeff}.out\"\n",
    "        \n",
    "        if not Path(model_path).exists():\n",
    "            ffm_model.fit(param, model_path)\n",
    "\n",
    "        self.models.append(model_path)\n",
    "        self.params.append(param)\n",
    "        \n",
    "        score = validate(model_path, self.valset)\n",
    "        \n",
    "        return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15b6306f-c0ad-4cfd-a38c-58617bcf4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d72b560-9c81-4280-bea5-a88bccfcddde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    k_     | l2_coeff_ |    lr_    |\n",
      "-------------------------------------------------------------\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=8_lr=0.20000000000000004_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 8\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.27 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.141904\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.52 (sec)\u001b[0m\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.1419  \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m-4.699   \u001b[0m | \u001b[0m-0.699   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=8_lr=0.20000000000000004_lambda=0.00020000000000000004.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 8\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.24 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.143262\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.49 (sec)\u001b[0m\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.1433  \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m-3.699   \u001b[0m | \u001b[0m-0.699   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=4_lr=0.20000000000000004_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 4\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.22 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.142414\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.49 (sec)\u001b[0m\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.1424  \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m-4.699   \u001b[0m | \u001b[0m-0.699   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for training task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/train.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of Field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 7.70 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel size: 7.17 MB\n",
      "\u001b[32m[------------] \u001b[0mTime cost for model initial: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n",
      "\u001b[32m[------------]\u001b[0m Epoch      Train log_loss       Test log_loss     Time cost (sec)\n",
      "\u001b[32m[ \u001b[0m   1%\u001b[32m      ]\u001b[0m     1            0.103785            0.154004                4.72\n",
      "\u001b[32m[ \u001b[0m   2%\u001b[32m      ]\u001b[0m     2            0.102101            0.152767                4.77\n",
      "\u001b[32m[ \u001b[0m   3%\u001b[32m      ]\u001b[0m     3            0.101695            0.152256                4.66\n",
      "\u001b[32m[ \u001b[0m   4%\u001b[32m      ]\u001b[0m     4            0.101485            0.151806                4.65\n",
      "\u001b[32m[ \u001b[0m   5%\u001b[32m      ]\u001b[0m     5            0.101329            0.151575                4.77\n",
      "\u001b[32m[ \u001b[0m   6%\u001b[32m      ]\u001b[0m     6            0.101214            0.151370                4.67\n",
      "\u001b[32m[ \u001b[0m   7%\u001b[32m      ]\u001b[0m     7            0.101112            0.151223                4.68\n",
      "\u001b[32m[ \u001b[0m   8%\u001b[32m      ]\u001b[0m     8            0.101025            0.151053                4.70\n",
      "\u001b[32m[ \u001b[0m   9%\u001b[32m      ]\u001b[0m     9            0.100955            0.150936                4.67\n",
      "\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m    10            0.100900            0.151002                4.72\n",
      "\u001b[32m[ \u001b[0m  11%\u001b[32m      ]\u001b[0m    11            0.100836            0.150822                4.67\n",
      "\u001b[32m[ \u001b[0m  12%\u001b[32m      ]\u001b[0m    12            0.100783            0.150743                4.74\n",
      "\u001b[32m[ \u001b[0m  13%\u001b[32m      ]\u001b[0m    13            0.100725            0.150766                4.67\n",
      "\u001b[32m[ \u001b[0m  14%\u001b[32m      ]\u001b[0m    14            0.100689            0.150736                4.66\n",
      "\u001b[32m[ \u001b[0m  15%\u001b[32m      ]\u001b[0m    15            0.100607            0.150594                5.26\n",
      "\u001b[32m[ \u001b[0m  16%\u001b[32m      ]\u001b[0m    16            0.100566            0.150565                5.00\n",
      "\u001b[32m[ \u001b[0m  17%\u001b[32m      ]\u001b[0m    17            0.100538            0.150505                5.12\n",
      "\u001b[32m[ \u001b[0m  18%\u001b[32m      ]\u001b[0m    18            0.100461            0.150509                4.74\n",
      "\u001b[32m[ \u001b[0m  19%\u001b[32m      ]\u001b[0m    19            0.100454            0.150471                4.70\n",
      "\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m    20            0.100398            0.150271                4.73\n",
      "\u001b[32m[ \u001b[0m  21%\u001b[32m      ]\u001b[0m    21            0.100369            0.150228                4.74\n",
      "\u001b[32m[ \u001b[0m  22%\u001b[32m      ]\u001b[0m    22            0.100378            0.150160                4.66\n",
      "\u001b[32m[ \u001b[0m  23%\u001b[32m      ]\u001b[0m    23            0.100344            0.150184                4.74\n",
      "\u001b[32m[ \u001b[0m  24%\u001b[32m      ]\u001b[0m    24            0.100334            0.150149                4.77\n",
      "\u001b[32m[ \u001b[0m  25%\u001b[32m      ]\u001b[0m    25            0.100280            0.150104                4.73\n",
      "\u001b[32m[ \u001b[0m  26%\u001b[32m      ]\u001b[0m    26            0.100243            0.150053                4.74\n",
      "\u001b[32m[ \u001b[0m  27%\u001b[32m      ]\u001b[0m    27            0.100257            0.150083                4.72\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    28            0.100224            0.149973                4.73\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    29            0.100219            0.149967                4.66\n",
      "\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m    30            0.100191            0.149893                4.76\n",
      "\u001b[32m[ \u001b[0m  31%\u001b[32m      ]\u001b[0m    31            0.100178            0.149880                4.73\n",
      "\u001b[32m[ \u001b[0m  32%\u001b[32m      ]\u001b[0m    32            0.100154            0.149799                4.70\n",
      "\u001b[32m[ \u001b[0m  33%\u001b[32m      ]\u001b[0m    33            0.100131            0.149735                4.65\n",
      "\u001b[32m[ \u001b[0m  34%\u001b[32m      ]\u001b[0m    34            0.100116            0.149681                4.68\n",
      "\u001b[32m[ \u001b[0m  35%\u001b[32m      ]\u001b[0m    35            0.100095            0.149667                4.71\n",
      "\u001b[32m[ \u001b[0m  36%\u001b[32m      ]\u001b[0m    36            0.100086            0.149634                4.72\n",
      "\u001b[32m[ \u001b[0m  37%\u001b[32m      ]\u001b[0m    37            0.100082            0.149610                4.66\n",
      "\u001b[32m[ \u001b[0m  38%\u001b[32m      ]\u001b[0m    38            0.100073            0.149566                4.64\n",
      "\u001b[32m[ \u001b[0m  39%\u001b[32m      ]\u001b[0m    39            0.100066            0.149557                4.59\n",
      "\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m    40            0.100051            0.149560                4.58\n",
      "\u001b[32m[ \u001b[0m  41%\u001b[32m      ]\u001b[0m    41            0.100037            0.149550                4.64\n",
      "\u001b[32m[ \u001b[0m  42%\u001b[32m      ]\u001b[0m    42            0.100021            0.149483                4.62\n",
      "\u001b[32m[ \u001b[0m  43%\u001b[32m      ]\u001b[0m    43            0.100015            0.149478                4.69\n",
      "\u001b[32m[ \u001b[0m  44%\u001b[32m      ]\u001b[0m    44            0.100012            0.149482                4.64\n",
      "\u001b[32m[ \u001b[0m  45%\u001b[32m      ]\u001b[0m    45            0.100014            0.149415                4.66\n",
      "\u001b[32m[ \u001b[0m  46%\u001b[32m      ]\u001b[0m    46            0.099995            0.149387                4.68\n",
      "\u001b[32m[ \u001b[0m  47%\u001b[32m      ]\u001b[0m    47            0.099994            0.149374                4.68\n",
      "\u001b[32m[ \u001b[0m  48%\u001b[32m      ]\u001b[0m    48            0.099982            0.149368                4.68\n",
      "\u001b[32m[ \u001b[0m  49%\u001b[32m      ]\u001b[0m    49            0.099983            0.149333                4.74\n",
      "\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m    50            0.099992            0.149297                4.68\n",
      "\u001b[32m[ \u001b[0m  51%\u001b[32m      ]\u001b[0m    51            0.099987            0.149289                4.64\n",
      "\u001b[32m[ \u001b[0m  52%\u001b[32m      ]\u001b[0m    52            0.099979            0.149272                4.69\n",
      "\u001b[32m[ \u001b[0m  53%\u001b[32m      ]\u001b[0m    53            0.099986            0.149257                4.67\n",
      "\u001b[32m[ \u001b[0m  54%\u001b[32m      ]\u001b[0m    54            0.099992            0.149264                5.15\n",
      "\u001b[32m[ \u001b[0m  55%\u001b[32m      ]\u001b[0m    55            0.099996            0.149192                4.89\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    56            0.099996            0.149219                4.69\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    57            0.100011            0.149174                4.81\n",
      "\u001b[32m[ \u001b[0m  57%\u001b[32m      ]\u001b[0m    58            0.100012            0.149124                5.13\n",
      "\u001b[32m[ \u001b[0m  59%\u001b[32m      ]\u001b[0m    59            0.100018            0.149122                4.59\n",
      "\u001b[32m[ \u001b[0m  60%\u001b[32m      ]\u001b[0m    60            0.100022            0.149139                4.66\n",
      "\u001b[32m[ \u001b[0m  61%\u001b[32m      ]\u001b[0m    61            0.100030            0.149145                4.74\n",
      "\u001b[32m[ \u001b[0m  62%\u001b[32m      ]\u001b[0m    62            0.100035            0.149130                4.76\n",
      "\u001b[32m[ \u001b[0m  63%\u001b[32m      ]\u001b[0m    63            0.100042            0.149075                4.71\n",
      "\u001b[32m[ \u001b[0m  64%\u001b[32m      ]\u001b[0m    64            0.100065            0.149082                4.71\n",
      "\u001b[32m[ \u001b[0m  65%\u001b[32m      ]\u001b[0m    65            0.100063            0.149088                4.70\n",
      "\u001b[32m[ \u001b[0m  66%\u001b[32m      ]\u001b[0m    66            0.100071            0.149090                4.70\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Early-stopping at epoch 63, best loss: 0.149075\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel file: ./model_k=13_lr=0.018239710656162565_lambda=0.001682711340723979.out\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving model: 0.01 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 319.93 (sec)\u001b[0m\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=13_lr=0.018239710656162565_lambda=0.001682711340723979.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 13\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.23 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.149075\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.47 (sec)\u001b[0m\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.1491  \u001b[0m | \u001b[0m 3.783   \u001b[0m | \u001b[0m-2.774   \u001b[0m | \u001b[0m-1.739   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for training task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/train.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of Field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 8.36 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel size: 5.40 MB\n",
      "\u001b[32m[------------] \u001b[0mTime cost for model initial: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n",
      "\u001b[32m[------------]\u001b[0m Epoch      Train log_loss       Test log_loss     Time cost (sec)\n",
      "\u001b[32m[ \u001b[0m   1%\u001b[32m      ]\u001b[0m     1            0.102039            0.147952                4.93\n",
      "\u001b[32m[ \u001b[0m   2%\u001b[32m      ]\u001b[0m     2            0.099624            0.146719                4.88\n",
      "\u001b[32m[ \u001b[0m   3%\u001b[32m      ]\u001b[0m     3            0.099040            0.146154                4.83\n",
      "\u001b[32m[ \u001b[0m   4%\u001b[32m      ]\u001b[0m     4            0.098718            0.145734                4.89\n",
      "\u001b[32m[ \u001b[0m   5%\u001b[32m      ]\u001b[0m     5            0.098477            0.145484                4.91\n",
      "\u001b[32m[ \u001b[0m   6%\u001b[32m      ]\u001b[0m     6            0.098306            0.145243                4.88\n",
      "\u001b[32m[ \u001b[0m   7%\u001b[32m      ]\u001b[0m     7            0.098147            0.145183                4.84\n",
      "\u001b[32m[ \u001b[0m   8%\u001b[32m      ]\u001b[0m     8            0.098049            0.144994                4.85\n",
      "\u001b[32m[ \u001b[0m   9%\u001b[32m      ]\u001b[0m     9            0.097938            0.144901                4.89\n",
      "\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m    10            0.097841            0.144797                4.89\n",
      "\u001b[32m[ \u001b[0m  11%\u001b[32m      ]\u001b[0m    11            0.097727            0.144626                4.87\n",
      "\u001b[32m[ \u001b[0m  12%\u001b[32m      ]\u001b[0m    12            0.097641            0.144542                4.85\n",
      "\u001b[32m[ \u001b[0m  13%\u001b[32m      ]\u001b[0m    13            0.097567            0.144437                4.80\n",
      "\u001b[32m[ \u001b[0m  14%\u001b[32m      ]\u001b[0m    14            0.097501            0.144377                4.88\n",
      "\u001b[32m[ \u001b[0m  15%\u001b[32m      ]\u001b[0m    15            0.097438            0.144295                4.85\n",
      "\u001b[32m[ \u001b[0m  16%\u001b[32m      ]\u001b[0m    16            0.097386            0.144316                4.88\n",
      "\u001b[32m[ \u001b[0m  17%\u001b[32m      ]\u001b[0m    17            0.097340            0.144246                4.98\n",
      "\u001b[32m[ \u001b[0m  18%\u001b[32m      ]\u001b[0m    18            0.097278            0.144185                4.87\n",
      "\u001b[32m[ \u001b[0m  19%\u001b[32m      ]\u001b[0m    19            0.097235            0.144156                4.89\n",
      "\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m    20            0.097177            0.144089                4.86\n",
      "\u001b[32m[ \u001b[0m  21%\u001b[32m      ]\u001b[0m    21            0.097140            0.144134                4.86\n",
      "\u001b[32m[ \u001b[0m  22%\u001b[32m      ]\u001b[0m    22            0.097094            0.144048                4.96\n",
      "\u001b[32m[ \u001b[0m  23%\u001b[32m      ]\u001b[0m    23            0.097056            0.144047                4.84\n",
      "\u001b[32m[ \u001b[0m  24%\u001b[32m      ]\u001b[0m    24            0.097008            0.144014                4.80\n",
      "\u001b[32m[ \u001b[0m  25%\u001b[32m      ]\u001b[0m    25            0.096958            0.143989                4.81\n",
      "\u001b[32m[ \u001b[0m  26%\u001b[32m      ]\u001b[0m    26            0.096932            0.143858                4.83\n",
      "\u001b[32m[ \u001b[0m  27%\u001b[32m      ]\u001b[0m    27            0.096885            0.143868                4.80\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    28            0.096840            0.143792                4.83\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    29            0.096807            0.143760                4.84\n",
      "\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m    30            0.096770            0.143740                4.93\n",
      "\u001b[32m[ \u001b[0m  31%\u001b[32m      ]\u001b[0m    31            0.096742            0.143706                4.81\n",
      "\u001b[32m[ \u001b[0m  32%\u001b[32m      ]\u001b[0m    32            0.096709            0.143680                4.81\n",
      "\u001b[32m[ \u001b[0m  33%\u001b[32m      ]\u001b[0m    33            0.096675            0.143619                4.81\n",
      "\u001b[32m[ \u001b[0m  34%\u001b[32m      ]\u001b[0m    34            0.096656            0.143588                4.82\n",
      "\u001b[32m[ \u001b[0m  35%\u001b[32m      ]\u001b[0m    35            0.096620            0.143569                4.80\n",
      "\u001b[32m[ \u001b[0m  36%\u001b[32m      ]\u001b[0m    36            0.096601            0.143530                4.82\n",
      "\u001b[32m[ \u001b[0m  37%\u001b[32m      ]\u001b[0m    37            0.096570            0.143518                4.81\n",
      "\u001b[32m[ \u001b[0m  38%\u001b[32m      ]\u001b[0m    38            0.096561            0.143492                4.81\n",
      "\u001b[32m[ \u001b[0m  39%\u001b[32m      ]\u001b[0m    39            0.096533            0.143475                4.80\n",
      "\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m    40            0.096509            0.143473                4.82\n",
      "\u001b[32m[ \u001b[0m  41%\u001b[32m      ]\u001b[0m    41            0.096477            0.143479                4.83\n",
      "\u001b[32m[ \u001b[0m  42%\u001b[32m      ]\u001b[0m    42            0.096471            0.143412                4.83\n",
      "\u001b[32m[ \u001b[0m  43%\u001b[32m      ]\u001b[0m    43            0.096439            0.143379                4.81\n",
      "\u001b[32m[ \u001b[0m  44%\u001b[32m      ]\u001b[0m    44            0.096424            0.143387                4.82\n",
      "\u001b[32m[ \u001b[0m  45%\u001b[32m      ]\u001b[0m    45            0.096395            0.143367                4.83\n",
      "\u001b[32m[ \u001b[0m  46%\u001b[32m      ]\u001b[0m    46            0.096377            0.143328                4.81\n",
      "\u001b[32m[ \u001b[0m  47%\u001b[32m      ]\u001b[0m    47            0.096363            0.143287                4.82\n",
      "\u001b[32m[ \u001b[0m  48%\u001b[32m      ]\u001b[0m    48            0.096347            0.143294                4.83\n",
      "\u001b[32m[ \u001b[0m  49%\u001b[32m      ]\u001b[0m    49            0.096329            0.143274                4.87\n",
      "\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m    50            0.096320            0.143262                4.81\n",
      "\u001b[32m[ \u001b[0m  51%\u001b[32m      ]\u001b[0m    51            0.096299            0.143267                4.83\n",
      "\u001b[32m[ \u001b[0m  52%\u001b[32m      ]\u001b[0m    52            0.096286            0.143225                4.82\n",
      "\u001b[32m[ \u001b[0m  53%\u001b[32m      ]\u001b[0m    53            0.096276            0.143217                4.82\n",
      "\u001b[32m[ \u001b[0m  54%\u001b[32m      ]\u001b[0m    54            0.096247            0.143215                4.82\n",
      "\u001b[32m[ \u001b[0m  55%\u001b[32m      ]\u001b[0m    55            0.096239            0.143206                4.89\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    56            0.096231            0.143181                4.86\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    57            0.096211            0.143175                4.86\n",
      "\u001b[32m[ \u001b[0m  57%\u001b[32m      ]\u001b[0m    58            0.096205            0.143164                4.82\n",
      "\u001b[32m[ \u001b[0m  59%\u001b[32m      ]\u001b[0m    59            0.096195            0.143163                4.84\n",
      "\u001b[32m[ \u001b[0m  60%\u001b[32m      ]\u001b[0m    60            0.096187            0.143158                4.83\n",
      "\u001b[32m[ \u001b[0m  61%\u001b[32m      ]\u001b[0m    61            0.096178            0.143168                4.82\n",
      "\u001b[32m[ \u001b[0m  62%\u001b[32m      ]\u001b[0m    62            0.096167            0.143139                4.80\n",
      "\u001b[32m[ \u001b[0m  63%\u001b[32m      ]\u001b[0m    63            0.096161            0.143143                4.79\n",
      "\u001b[32m[ \u001b[0m  64%\u001b[32m      ]\u001b[0m    64            0.096148            0.143126                4.80\n",
      "\u001b[32m[ \u001b[0m  65%\u001b[32m      ]\u001b[0m    65            0.096134            0.143149                4.80\n",
      "\u001b[32m[ \u001b[0m  66%\u001b[32m      ]\u001b[0m    66            0.096130            0.143126                4.82\n",
      "\u001b[32m[ \u001b[0m  67%\u001b[32m      ]\u001b[0m    67            0.096115            0.143147                4.83\n",
      "\u001b[32m[ \u001b[0m  68%\u001b[32m      ]\u001b[0m    68            0.096103            0.143119                4.83\n",
      "\u001b[32m[ \u001b[0m  69%\u001b[32m      ]\u001b[0m    69            0.096096            0.143139                4.83\n",
      "\u001b[32m[ \u001b[0m  70%\u001b[32m      ]\u001b[0m    70            0.096099            0.143141                4.83\n",
      "\u001b[32m[ \u001b[0m  71%\u001b[32m      ]\u001b[0m    71            0.096087            0.143089                4.82\n",
      "\u001b[32m[ \u001b[0m  72%\u001b[32m      ]\u001b[0m    72            0.096086            0.143093                4.82\n",
      "\u001b[32m[ \u001b[0m  73%\u001b[32m      ]\u001b[0m    73            0.096076            0.143084                4.84\n",
      "\u001b[32m[ \u001b[0m  74%\u001b[32m      ]\u001b[0m    74            0.096057            0.143064                4.85\n",
      "\u001b[32m[ \u001b[0m  75%\u001b[32m      ]\u001b[0m    75            0.096054            0.143086                4.87\n",
      "\u001b[32m[ \u001b[0m  76%\u001b[32m      ]\u001b[0m    76            0.096049            0.143065                4.81\n",
      "\u001b[32m[ \u001b[0m  77%\u001b[32m      ]\u001b[0m    77            0.096030            0.143079                4.82\n",
      "\u001b[32m[ \u001b[0m  78%\u001b[32m      ]\u001b[0m    78            0.096023            0.143082                4.82\n",
      "\u001b[32m[ \u001b[0m  79%\u001b[32m      ]\u001b[0m    79            0.096019            0.143068                4.85\n",
      "\u001b[32m[ \u001b[0m  80%\u001b[32m      ]\u001b[0m    80            0.096016            0.143066                4.83\n",
      "\u001b[32m[ \u001b[0m  81%\u001b[32m      ]\u001b[0m    81            0.096005            0.143039                4.85\n",
      "\u001b[32m[ \u001b[0m  82%\u001b[32m      ]\u001b[0m    82            0.096006            0.143038                4.80\n",
      "\u001b[32m[ \u001b[0m  83%\u001b[32m      ]\u001b[0m    83            0.095999            0.143022                4.81\n",
      "\u001b[32m[ \u001b[0m  84%\u001b[32m      ]\u001b[0m    84            0.095995            0.143046                4.83\n",
      "\u001b[32m[ \u001b[0m  85%\u001b[32m      ]\u001b[0m    85            0.095979            0.143044                4.87\n",
      "\u001b[32m[ \u001b[0m  86%\u001b[32m      ]\u001b[0m    86            0.095982            0.143041                4.88\n",
      "\u001b[32m[ \u001b[0m  87%\u001b[32m      ]\u001b[0m    87            0.095971            0.143042                4.80\n",
      "\u001b[32m[ \u001b[0m  88%\u001b[32m      ]\u001b[0m    88            0.095969            0.143057                4.81\n",
      "\u001b[32m[ \u001b[0m  89%\u001b[32m      ]\u001b[0m    89            0.095968            0.143044                4.80\n",
      "\u001b[32m[ \u001b[0m  90%\u001b[32m      ]\u001b[0m    90            0.095958            0.143049                4.81\n",
      "\u001b[32m[ \u001b[0m  91%\u001b[32m      ]\u001b[0m    91            0.095955            0.143077                4.80\n",
      "\u001b[32m[ \u001b[0m  92%\u001b[32m      ]\u001b[0m    92            0.095954            0.143088                4.84\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Early-stopping at epoch 83, best loss: 0.143022\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel file: ./model_k=10_lr=0.013633150644635009_lambda=9.098523214437794e-05.out\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving model: 0.01 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 453.55 (sec)\u001b[0m\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=10_lr=0.013633150644635009_lambda=9.098523214437794e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 10\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.23 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.143022\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.48 (sec)\u001b[0m\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.143   \u001b[0m | \u001b[0m 3.38    \u001b[0m | \u001b[0m-4.041   \u001b[0m | \u001b[0m-1.865   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for training task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/train.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of Field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 7.95 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel size: 3.63 MB\n",
      "\u001b[32m[------------] \u001b[0mTime cost for model initial: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n",
      "\u001b[32m[------------]\u001b[0m Epoch      Train log_loss       Test log_loss     Time cost (sec)\n",
      "\u001b[32m[ \u001b[0m   1%\u001b[32m      ]\u001b[0m     1            0.101317            0.146880                3.78\n",
      "\u001b[32m[ \u001b[0m   2%\u001b[32m      ]\u001b[0m     2            0.099113            0.145838                3.82\n",
      "\u001b[32m[ \u001b[0m   3%\u001b[32m      ]\u001b[0m     3            0.098600            0.145253                3.75\n",
      "\u001b[32m[ \u001b[0m   4%\u001b[32m      ]\u001b[0m     4            0.098299            0.145145                3.81\n",
      "\u001b[32m[ \u001b[0m   5%\u001b[32m      ]\u001b[0m     5            0.098074            0.144819                3.72\n",
      "\u001b[32m[ \u001b[0m   6%\u001b[32m      ]\u001b[0m     6            0.097886            0.144712                3.74\n",
      "\u001b[32m[ \u001b[0m   7%\u001b[32m      ]\u001b[0m     7            0.097723            0.144573                3.75\n",
      "\u001b[32m[ \u001b[0m   8%\u001b[32m      ]\u001b[0m     8            0.097587            0.144502                3.74\n",
      "\u001b[32m[ \u001b[0m   9%\u001b[32m      ]\u001b[0m     9            0.097475            0.144358                3.75\n",
      "\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m    10            0.097349            0.144195                3.73\n",
      "\u001b[32m[ \u001b[0m  11%\u001b[32m      ]\u001b[0m    11            0.097257            0.144082                3.73\n",
      "\u001b[32m[ \u001b[0m  12%\u001b[32m      ]\u001b[0m    12            0.097160            0.144017                3.73\n",
      "\u001b[32m[ \u001b[0m  13%\u001b[32m      ]\u001b[0m    13            0.097082            0.144064                3.75\n",
      "\u001b[32m[ \u001b[0m  14%\u001b[32m      ]\u001b[0m    14            0.096984            0.143822                3.75\n",
      "\u001b[32m[ \u001b[0m  15%\u001b[32m      ]\u001b[0m    15            0.096913            0.143825                3.75\n",
      "\u001b[32m[ \u001b[0m  16%\u001b[32m      ]\u001b[0m    16            0.096831            0.143716                3.76\n",
      "\u001b[32m[ \u001b[0m  17%\u001b[32m      ]\u001b[0m    17            0.096772            0.143639                3.95\n",
      "\u001b[32m[ \u001b[0m  18%\u001b[32m      ]\u001b[0m    18            0.096699            0.143608                3.78\n",
      "\u001b[32m[ \u001b[0m  19%\u001b[32m      ]\u001b[0m    19            0.096637            0.143551                3.80\n",
      "\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m    20            0.096605            0.143516                4.07\n",
      "\u001b[32m[ \u001b[0m  21%\u001b[32m      ]\u001b[0m    21            0.096541            0.143456                3.86\n",
      "\u001b[32m[ \u001b[0m  22%\u001b[32m      ]\u001b[0m    22            0.096498            0.143446                3.78\n",
      "\u001b[32m[ \u001b[0m  23%\u001b[32m      ]\u001b[0m    23            0.096450            0.143366                3.77\n",
      "\u001b[32m[ \u001b[0m  24%\u001b[32m      ]\u001b[0m    24            0.096413            0.143302                4.06\n",
      "\u001b[32m[ \u001b[0m  25%\u001b[32m      ]\u001b[0m    25            0.096378            0.143294                4.00\n",
      "\u001b[32m[ \u001b[0m  26%\u001b[32m      ]\u001b[0m    26            0.096335            0.143251                3.89\n",
      "\u001b[32m[ \u001b[0m  27%\u001b[32m      ]\u001b[0m    27            0.096307            0.143168                3.75\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    28            0.096273            0.143181                3.74\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    29            0.096242            0.143123                3.73\n",
      "\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m    30            0.096232            0.143119                3.72\n",
      "\u001b[32m[ \u001b[0m  31%\u001b[32m      ]\u001b[0m    31            0.096208            0.143039                3.81\n",
      "\u001b[32m[ \u001b[0m  32%\u001b[32m      ]\u001b[0m    32            0.096199            0.143048                3.76\n",
      "\u001b[32m[ \u001b[0m  33%\u001b[32m      ]\u001b[0m    33            0.096174            0.143045                3.75\n",
      "\u001b[32m[ \u001b[0m  34%\u001b[32m      ]\u001b[0m    34            0.096161            0.143011                3.73\n",
      "\u001b[32m[ \u001b[0m  35%\u001b[32m      ]\u001b[0m    35            0.096149            0.143042                3.83\n",
      "\u001b[32m[ \u001b[0m  36%\u001b[32m      ]\u001b[0m    36            0.096142            0.142995                3.82\n",
      "\u001b[32m[ \u001b[0m  37%\u001b[32m      ]\u001b[0m    37            0.096118            0.142984                3.81\n",
      "\u001b[32m[ \u001b[0m  38%\u001b[32m      ]\u001b[0m    38            0.096098            0.142983                3.78\n",
      "\u001b[32m[ \u001b[0m  39%\u001b[32m      ]\u001b[0m    39            0.096066            0.142951                3.76\n",
      "\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m    40            0.096042            0.142886                3.81\n",
      "\u001b[32m[ \u001b[0m  41%\u001b[32m      ]\u001b[0m    41            0.096012            0.142938                3.87\n",
      "\u001b[32m[ \u001b[0m  42%\u001b[32m      ]\u001b[0m    42            0.095994            0.142870                3.76\n",
      "\u001b[32m[ \u001b[0m  43%\u001b[32m      ]\u001b[0m    43            0.095975            0.142927                3.76\n",
      "\u001b[32m[ \u001b[0m  44%\u001b[32m      ]\u001b[0m    44            0.095959            0.142897                3.88\n",
      "\u001b[32m[ \u001b[0m  45%\u001b[32m      ]\u001b[0m    45            0.095940            0.142911                3.79\n",
      "\u001b[32m[ \u001b[0m  46%\u001b[32m      ]\u001b[0m    46            0.095913            0.142904                3.75\n",
      "\u001b[32m[ \u001b[0m  47%\u001b[32m      ]\u001b[0m    47            0.095895            0.142877                3.75\n",
      "\u001b[32m[ \u001b[0m  48%\u001b[32m      ]\u001b[0m    48            0.095888            0.142896                3.75\n",
      "\u001b[32m[ \u001b[0m  49%\u001b[32m      ]\u001b[0m    49            0.095878            0.142890                3.76\n",
      "\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m    50            0.095880            0.142831                3.76\n",
      "\u001b[32m[ \u001b[0m  51%\u001b[32m      ]\u001b[0m    51            0.095857            0.142879                3.75\n",
      "\u001b[32m[ \u001b[0m  52%\u001b[32m      ]\u001b[0m    52            0.095846            0.142853                3.81\n",
      "\u001b[32m[ \u001b[0m  53%\u001b[32m      ]\u001b[0m    53            0.095836            0.142841                3.73\n",
      "\u001b[32m[ \u001b[0m  54%\u001b[32m      ]\u001b[0m    54            0.095833            0.142864                3.77\n",
      "\u001b[32m[ \u001b[0m  55%\u001b[32m      ]\u001b[0m    55            0.095807            0.142847                3.82\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    56            0.095795            0.142838                3.78\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    57            0.095773            0.142799                3.76\n",
      "\u001b[32m[ \u001b[0m  57%\u001b[32m      ]\u001b[0m    58            0.095765            0.142804                3.79\n",
      "\u001b[32m[ \u001b[0m  59%\u001b[32m      ]\u001b[0m    59            0.095745            0.142802                3.79\n",
      "\u001b[32m[ \u001b[0m  60%\u001b[32m      ]\u001b[0m    60            0.095740            0.142787                3.88\n",
      "\u001b[32m[ \u001b[0m  61%\u001b[32m      ]\u001b[0m    61            0.095733            0.142884                3.94\n",
      "\u001b[32m[ \u001b[0m  62%\u001b[32m      ]\u001b[0m    62            0.095713            0.142821                3.80\n",
      "\u001b[32m[ \u001b[0m  63%\u001b[32m      ]\u001b[0m    63            0.095701            0.142791                3.77\n",
      "\u001b[32m[ \u001b[0m  64%\u001b[32m      ]\u001b[0m    64            0.095692            0.142796                3.76\n",
      "\u001b[32m[ \u001b[0m  65%\u001b[32m      ]\u001b[0m    65            0.095681            0.142821                3.77\n",
      "\u001b[32m[ \u001b[0m  66%\u001b[32m      ]\u001b[0m    66            0.095673            0.142796                3.77\n",
      "\u001b[32m[ \u001b[0m  67%\u001b[32m      ]\u001b[0m    67            0.095657            0.142794                3.81\n",
      "\u001b[32m[ \u001b[0m  68%\u001b[32m      ]\u001b[0m    68            0.095656            0.142796                3.79\n",
      "\u001b[32m[ \u001b[0m  69%\u001b[32m      ]\u001b[0m    69            0.095636            0.142783                3.78\n",
      "\u001b[32m[ \u001b[0m  70%\u001b[32m      ]\u001b[0m    70            0.095632            0.142772                3.79\n",
      "\u001b[32m[ \u001b[0m  71%\u001b[32m      ]\u001b[0m    71            0.095619            0.142778                3.79\n",
      "\u001b[32m[ \u001b[0m  72%\u001b[32m      ]\u001b[0m    72            0.095617            0.142754                3.80\n",
      "\u001b[32m[ \u001b[0m  73%\u001b[32m      ]\u001b[0m    73            0.095599            0.142733                3.80\n",
      "\u001b[32m[ \u001b[0m  74%\u001b[32m      ]\u001b[0m    74            0.095600            0.142748                3.89\n",
      "\u001b[32m[ \u001b[0m  75%\u001b[32m      ]\u001b[0m    75            0.095594            0.142770                3.78\n",
      "\u001b[32m[ \u001b[0m  76%\u001b[32m      ]\u001b[0m    76            0.095578            0.142737                3.78\n",
      "\u001b[32m[ \u001b[0m  77%\u001b[32m      ]\u001b[0m    77            0.095564            0.142730                3.80\n",
      "\u001b[32m[ \u001b[0m  78%\u001b[32m      ]\u001b[0m    78            0.095556            0.142730                3.78\n",
      "\u001b[32m[ \u001b[0m  79%\u001b[32m      ]\u001b[0m    79            0.095549            0.142714                3.76\n",
      "\u001b[32m[ \u001b[0m  80%\u001b[32m      ]\u001b[0m    80            0.095546            0.142744                3.80\n",
      "\u001b[32m[ \u001b[0m  81%\u001b[32m      ]\u001b[0m    81            0.095530            0.142684                3.76\n",
      "\u001b[32m[ \u001b[0m  82%\u001b[32m      ]\u001b[0m    82            0.095538            0.142729                3.76\n",
      "\u001b[32m[ \u001b[0m  83%\u001b[32m      ]\u001b[0m    83            0.095530            0.142717                3.75\n",
      "\u001b[32m[ \u001b[0m  84%\u001b[32m      ]\u001b[0m    84            0.095516            0.142702                3.76\n",
      "\u001b[32m[ \u001b[0m  85%\u001b[32m      ]\u001b[0m    85            0.095503            0.142703                3.76\n",
      "\u001b[32m[ \u001b[0m  86%\u001b[32m      ]\u001b[0m    86            0.095504            0.142720                3.75\n",
      "\u001b[32m[ \u001b[0m  87%\u001b[32m      ]\u001b[0m    87            0.095502            0.142706                3.79\n",
      "\u001b[32m[ \u001b[0m  88%\u001b[32m      ]\u001b[0m    88            0.095494            0.142707                3.76\n",
      "\u001b[32m[ \u001b[0m  89%\u001b[32m      ]\u001b[0m    89            0.095493            0.142704                3.77\n",
      "\u001b[32m[ \u001b[0m  90%\u001b[32m      ]\u001b[0m    90            0.095478            0.142695                3.77\n",
      "\u001b[32m[ \u001b[0m  91%\u001b[32m      ]\u001b[0m    91            0.095475            0.142704                3.74\n",
      "\u001b[32m[ \u001b[0m  92%\u001b[32m      ]\u001b[0m    92            0.095466            0.142669                3.77\n",
      "\u001b[32m[ \u001b[0m  93%\u001b[32m      ]\u001b[0m    93            0.095462            0.142719                3.79\n",
      "\u001b[32m[ \u001b[0m  94%\u001b[32m      ]\u001b[0m    94            0.095461            0.142705                3.77\n",
      "\u001b[32m[ \u001b[0m  95%\u001b[32m      ]\u001b[0m    95            0.095457            0.142705                3.76\n",
      "\u001b[32m[ \u001b[0m  96%\u001b[32m      ]\u001b[0m    96            0.095447            0.142703                3.76\n",
      "\u001b[32m[ \u001b[0m  97%\u001b[32m      ]\u001b[0m    97            0.095427            0.142687                3.72\n",
      "\u001b[32m[ \u001b[0m  98%\u001b[32m      ]\u001b[0m    98            0.095432            0.142686                3.76\n",
      "\u001b[32m[ \u001b[0m  99%\u001b[32m      ]\u001b[0m    99            0.095430            0.142682                3.76\n",
      "\u001b[32m[ \u001b[0m 100%\u001b[32m      ]\u001b[0m   100            0.095423            0.142689                3.78\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Early-stopping at epoch 92, best loss: 0.142669\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel file: ./model_k=6_lr=0.019829923480712375_lambda=3.637481691522213e-05.out\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving model: 0.01 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 386.72 (sec)\u001b[0m\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=6_lr=0.019829923480712375_lambda=3.637481691522213e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 6\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.25 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.142669\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.50 (sec)\u001b[0m\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.1427  \u001b[0m | \u001b[0m 2.653   \u001b[0m | \u001b[0m-4.439   \u001b[0m | \u001b[0m-1.703   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for training task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/train.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of Field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 8.41 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel size: 5.40 MB\n",
      "\u001b[32m[------------] \u001b[0mTime cost for model initial: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n",
      "\u001b[32m[------------]\u001b[0m Epoch      Train log_loss       Test log_loss     Time cost (sec)\n",
      "\u001b[32m[ \u001b[0m   1%\u001b[32m      ]\u001b[0m     1            0.103904            0.150267                4.85\n",
      "\u001b[32m[ \u001b[0m   2%\u001b[32m      ]\u001b[0m     2            0.100718            0.148419                4.85\n",
      "\u001b[32m[ \u001b[0m   3%\u001b[32m      ]\u001b[0m     3            0.100111            0.147596                4.84\n",
      "\u001b[32m[ \u001b[0m   4%\u001b[32m      ]\u001b[0m     4            0.099741            0.147040                4.83\n",
      "\u001b[32m[ \u001b[0m   5%\u001b[32m      ]\u001b[0m     5            0.099512            0.146676                4.85\n",
      "\u001b[32m[ \u001b[0m   6%\u001b[32m      ]\u001b[0m     6            0.099324            0.146392                4.85\n",
      "\u001b[32m[ \u001b[0m   7%\u001b[32m      ]\u001b[0m     7            0.099165            0.146210                4.85\n",
      "\u001b[32m[ \u001b[0m   8%\u001b[32m      ]\u001b[0m     8            0.099020            0.146019                4.84\n",
      "\u001b[32m[ \u001b[0m   9%\u001b[32m      ]\u001b[0m     9            0.098904            0.145858                4.84\n",
      "\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m    10            0.098800            0.145765                4.84\n",
      "\u001b[32m[ \u001b[0m  11%\u001b[32m      ]\u001b[0m    11            0.098716            0.145676                4.87\n",
      "\u001b[32m[ \u001b[0m  12%\u001b[32m      ]\u001b[0m    12            0.098636            0.145561                4.90\n",
      "\u001b[32m[ \u001b[0m  13%\u001b[32m      ]\u001b[0m    13            0.098566            0.145476                4.86\n",
      "\u001b[32m[ \u001b[0m  14%\u001b[32m      ]\u001b[0m    14            0.098481            0.145388                4.94\n",
      "\u001b[32m[ \u001b[0m  15%\u001b[32m      ]\u001b[0m    15            0.098431            0.145331                4.82\n",
      "\u001b[32m[ \u001b[0m  16%\u001b[32m      ]\u001b[0m    16            0.098376            0.145265                4.86\n",
      "\u001b[32m[ \u001b[0m  17%\u001b[32m      ]\u001b[0m    17            0.098336            0.145199                4.91\n",
      "\u001b[32m[ \u001b[0m  18%\u001b[32m      ]\u001b[0m    18            0.098289            0.145156                4.84\n",
      "\u001b[32m[ \u001b[0m  19%\u001b[32m      ]\u001b[0m    19            0.098243            0.145089                4.84\n",
      "\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m    20            0.098207            0.145062                4.85\n",
      "\u001b[32m[ \u001b[0m  21%\u001b[32m      ]\u001b[0m    21            0.098146            0.145005                4.84\n",
      "\u001b[32m[ \u001b[0m  22%\u001b[32m      ]\u001b[0m    22            0.098110            0.144986                4.84\n",
      "\u001b[32m[ \u001b[0m  23%\u001b[32m      ]\u001b[0m    23            0.098095            0.144952                4.84\n",
      "\u001b[32m[ \u001b[0m  24%\u001b[32m      ]\u001b[0m    24            0.098045            0.144901                4.83\n",
      "\u001b[32m[ \u001b[0m  25%\u001b[32m      ]\u001b[0m    25            0.098017            0.144867                4.84\n",
      "\u001b[32m[ \u001b[0m  26%\u001b[32m      ]\u001b[0m    26            0.097973            0.144827                4.82\n",
      "\u001b[32m[ \u001b[0m  27%\u001b[32m      ]\u001b[0m    27            0.097959            0.144800                4.84\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    28            0.097926            0.144742                4.85\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    29            0.097907            0.144730                4.84\n",
      "\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m    30            0.097879            0.144694                4.84\n",
      "\u001b[32m[ \u001b[0m  31%\u001b[32m      ]\u001b[0m    31            0.097849            0.144677                4.83\n",
      "\u001b[32m[ \u001b[0m  32%\u001b[32m      ]\u001b[0m    32            0.097826            0.144690                4.79\n",
      "\u001b[32m[ \u001b[0m  33%\u001b[32m      ]\u001b[0m    33            0.097789            0.144652                4.85\n",
      "\u001b[32m[ \u001b[0m  34%\u001b[32m      ]\u001b[0m    34            0.097776            0.144632                4.82\n",
      "\u001b[32m[ \u001b[0m  35%\u001b[32m      ]\u001b[0m    35            0.097756            0.144611                4.84\n",
      "\u001b[32m[ \u001b[0m  36%\u001b[32m      ]\u001b[0m    36            0.097737            0.144569                4.83\n",
      "\u001b[32m[ \u001b[0m  37%\u001b[32m      ]\u001b[0m    37            0.097715            0.144552                4.83\n",
      "\u001b[32m[ \u001b[0m  38%\u001b[32m      ]\u001b[0m    38            0.097693            0.144497                4.82\n",
      "\u001b[32m[ \u001b[0m  39%\u001b[32m      ]\u001b[0m    39            0.097670            0.144489                4.82\n",
      "\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m    40            0.097654            0.144486                4.82\n",
      "\u001b[32m[ \u001b[0m  41%\u001b[32m      ]\u001b[0m    41            0.097631            0.144480                4.83\n",
      "\u001b[32m[ \u001b[0m  42%\u001b[32m      ]\u001b[0m    42            0.097614            0.144478                4.82\n",
      "\u001b[32m[ \u001b[0m  43%\u001b[32m      ]\u001b[0m    43            0.097601            0.144449                4.78\n",
      "\u001b[32m[ \u001b[0m  44%\u001b[32m      ]\u001b[0m    44            0.097580            0.144438                4.82\n",
      "\u001b[32m[ \u001b[0m  45%\u001b[32m      ]\u001b[0m    45            0.097565            0.144452                4.83\n",
      "\u001b[32m[ \u001b[0m  46%\u001b[32m      ]\u001b[0m    46            0.097542            0.144444                4.81\n",
      "\u001b[32m[ \u001b[0m  47%\u001b[32m      ]\u001b[0m    47            0.097526            0.144434                4.83\n",
      "\u001b[32m[ \u001b[0m  48%\u001b[32m      ]\u001b[0m    48            0.097500            0.144423                4.82\n",
      "\u001b[32m[ \u001b[0m  49%\u001b[32m      ]\u001b[0m    49            0.097492            0.144415                4.84\n",
      "\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m    50            0.097479            0.144413                4.82\n",
      "\u001b[32m[ \u001b[0m  51%\u001b[32m      ]\u001b[0m    51            0.097460            0.144540                5.03\n",
      "\u001b[32m[ \u001b[0m  52%\u001b[32m      ]\u001b[0m    52            0.097373            0.144523                4.93\n",
      "\u001b[32m[ \u001b[0m  53%\u001b[32m      ]\u001b[0m    53            0.097336            0.144452                4.83\n",
      "\u001b[32m[ \u001b[0m  54%\u001b[32m      ]\u001b[0m    54            0.097340            0.144418                4.82\n",
      "\u001b[32m[ \u001b[0m  55%\u001b[32m      ]\u001b[0m    55            0.097289            0.144386                4.82\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    56            0.097291            0.144362                4.79\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    57            0.097256            0.144339                4.82\n",
      "\u001b[32m[ \u001b[0m  57%\u001b[32m      ]\u001b[0m    58            0.097201            0.144339                4.78\n",
      "\u001b[32m[ \u001b[0m  59%\u001b[32m      ]\u001b[0m    59            0.097192            0.144295                4.85\n",
      "\u001b[32m[ \u001b[0m  60%\u001b[32m      ]\u001b[0m    60            0.097221            0.144271                4.78\n",
      "\u001b[32m[ \u001b[0m  61%\u001b[32m      ]\u001b[0m    61            0.097193            0.144240                4.84\n",
      "\u001b[32m[ \u001b[0m  62%\u001b[32m      ]\u001b[0m    62            0.097176            0.144212                4.92\n",
      "\u001b[32m[ \u001b[0m  63%\u001b[32m      ]\u001b[0m    63            0.097156            0.144194                4.82\n",
      "\u001b[32m[ \u001b[0m  64%\u001b[32m      ]\u001b[0m    64            0.097134            0.144177                4.88\n",
      "\u001b[32m[ \u001b[0m  65%\u001b[32m      ]\u001b[0m    65            0.097116            0.144157                4.86\n",
      "\u001b[32m[ \u001b[0m  66%\u001b[32m      ]\u001b[0m    66            0.097092            0.144133                4.85\n",
      "\u001b[32m[ \u001b[0m  67%\u001b[32m      ]\u001b[0m    67            0.097074            0.144112                4.83\n",
      "\u001b[32m[ \u001b[0m  68%\u001b[32m      ]\u001b[0m    68            0.097051            0.144086                4.83\n",
      "\u001b[32m[ \u001b[0m  69%\u001b[32m      ]\u001b[0m    69            0.097042            0.144061                4.83\n",
      "\u001b[32m[ \u001b[0m  70%\u001b[32m      ]\u001b[0m    70            0.097031            0.144044                4.82\n",
      "\u001b[32m[ \u001b[0m  71%\u001b[32m      ]\u001b[0m    71            0.097012            0.144054                4.84\n",
      "\u001b[32m[ \u001b[0m  72%\u001b[32m      ]\u001b[0m    72            0.096995            0.144023                4.84\n",
      "\u001b[32m[ \u001b[0m  73%\u001b[32m      ]\u001b[0m    73            0.096987            0.144038                4.84\n",
      "\u001b[32m[ \u001b[0m  74%\u001b[32m      ]\u001b[0m    74            0.096972            0.144025                4.83\n",
      "\u001b[32m[ \u001b[0m  75%\u001b[32m      ]\u001b[0m    75            0.096957            0.144025                4.84\n",
      "\u001b[32m[ \u001b[0m  76%\u001b[32m      ]\u001b[0m    76            0.096952            0.144015                4.82\n",
      "\u001b[32m[ \u001b[0m  77%\u001b[32m      ]\u001b[0m    77            0.096936            0.144002                4.84\n",
      "\u001b[32m[ \u001b[0m  78%\u001b[32m      ]\u001b[0m    78            0.096920            0.144003                4.93\n",
      "\u001b[32m[ \u001b[0m  79%\u001b[32m      ]\u001b[0m    79            0.096902            0.143992                4.89\n",
      "\u001b[32m[ \u001b[0m  80%\u001b[32m      ]\u001b[0m    80            0.096896            0.143999                4.86\n",
      "\u001b[32m[ \u001b[0m  81%\u001b[32m      ]\u001b[0m    81            0.096875            0.143987                4.87\n",
      "\u001b[32m[ \u001b[0m  82%\u001b[32m      ]\u001b[0m    82            0.096867            0.143994                4.84\n",
      "\u001b[32m[ \u001b[0m  83%\u001b[32m      ]\u001b[0m    83            0.096845            0.143987                4.85\n",
      "\u001b[32m[ \u001b[0m  84%\u001b[32m      ]\u001b[0m    84            0.096840            0.143968                4.84\n",
      "\u001b[32m[ \u001b[0m  85%\u001b[32m      ]\u001b[0m    85            0.096827            0.143959                4.84\n",
      "\u001b[32m[ \u001b[0m  86%\u001b[32m      ]\u001b[0m    86            0.096813            0.143940                4.84\n",
      "\u001b[32m[ \u001b[0m  87%\u001b[32m      ]\u001b[0m    87            0.096802            0.143934                4.83\n",
      "\u001b[32m[ \u001b[0m  88%\u001b[32m      ]\u001b[0m    88            0.096786            0.143948                4.84\n",
      "\u001b[32m[ \u001b[0m  89%\u001b[32m      ]\u001b[0m    89            0.096774            0.143939                4.85\n",
      "\u001b[32m[ \u001b[0m  90%\u001b[32m      ]\u001b[0m    90            0.096764            0.143907                4.83\n",
      "\u001b[32m[ \u001b[0m  91%\u001b[32m      ]\u001b[0m    91            0.096760            0.143897                4.87\n",
      "\u001b[32m[ \u001b[0m  92%\u001b[32m      ]\u001b[0m    92            0.096750            0.143903                4.82\n",
      "\u001b[32m[ \u001b[0m  93%\u001b[32m      ]\u001b[0m    93            0.096738            0.143890                4.86\n",
      "\u001b[32m[ \u001b[0m  94%\u001b[32m      ]\u001b[0m    94            0.096728            0.143894                4.89\n",
      "\u001b[32m[ \u001b[0m  95%\u001b[32m      ]\u001b[0m    95            0.096722            0.143889                4.88\n",
      "\u001b[32m[ \u001b[0m  96%\u001b[32m      ]\u001b[0m    96            0.096710            0.143882                4.84\n",
      "\u001b[32m[ \u001b[0m  97%\u001b[32m      ]\u001b[0m    97            0.096709            0.143888                4.86\n",
      "\u001b[32m[ \u001b[0m  98%\u001b[32m      ]\u001b[0m    98            0.096695            0.143877                4.84\n",
      "\u001b[32m[ \u001b[0m  99%\u001b[32m      ]\u001b[0m    99            0.096685            0.143871                4.84\n",
      "\u001b[32m[ \u001b[0m 100%\u001b[32m      ]\u001b[0m   100            0.096675            0.143871                4.84\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel file: ./model_k=9_lr=0.007500353216451047_lambda=0.00012527174573451596.out\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving model: 0.01 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 492.89 (sec)\u001b[0m\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=9_lr=0.007500353216451047_lambda=0.00012527174573451596.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 9\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.23 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.143870\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.47 (sec)\u001b[0m\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.1439  \u001b[0m | \u001b[0m 3.174   \u001b[0m | \u001b[0m-3.902   \u001b[0m | \u001b[0m-2.125   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for training task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/train.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of Field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 8.74 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel size: 1.86 MB\n",
      "\u001b[32m[------------] \u001b[0mTime cost for model initial: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n",
      "\u001b[32m[------------]\u001b[0m Epoch      Train log_loss       Test log_loss     Time cost (sec)\n",
      "\u001b[32m[ \u001b[0m   1%\u001b[32m      ]\u001b[0m     1            0.103813            0.153047                3.81\n",
      "\u001b[32m[ \u001b[0m   2%\u001b[32m      ]\u001b[0m     2            0.101826            0.151695                3.77\n",
      "\u001b[32m[ \u001b[0m   3%\u001b[32m      ]\u001b[0m     3            0.101342            0.151082                3.79\n",
      "\u001b[32m[ \u001b[0m   4%\u001b[32m      ]\u001b[0m     4            0.101069            0.150606                3.87\n",
      "\u001b[32m[ \u001b[0m   5%\u001b[32m      ]\u001b[0m     5            0.100862            0.150106                3.80\n",
      "\u001b[32m[ \u001b[0m   6%\u001b[32m      ]\u001b[0m     6            0.100737            0.149884                3.79\n",
      "\u001b[32m[ \u001b[0m   7%\u001b[32m      ]\u001b[0m     7            0.100613            0.149631                3.77\n",
      "\u001b[32m[ \u001b[0m   8%\u001b[32m      ]\u001b[0m     8            0.100510            0.149536                3.77\n",
      "\u001b[32m[ \u001b[0m   9%\u001b[32m      ]\u001b[0m     9            0.100435            0.149418                3.77\n",
      "\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m    10            0.100359            0.149317                3.77\n",
      "\u001b[32m[ \u001b[0m  11%\u001b[32m      ]\u001b[0m    11            0.100294            0.149164                3.75\n",
      "\u001b[32m[ \u001b[0m  12%\u001b[32m      ]\u001b[0m    12            0.100234            0.149018                3.76\n",
      "\u001b[32m[ \u001b[0m  13%\u001b[32m      ]\u001b[0m    13            0.100165            0.149011                3.76\n",
      "\u001b[32m[ \u001b[0m  14%\u001b[32m      ]\u001b[0m    14            0.100116            0.148991                3.77\n",
      "\u001b[32m[ \u001b[0m  15%\u001b[32m      ]\u001b[0m    15            0.100073            0.148903                3.77\n",
      "\u001b[32m[ \u001b[0m  16%\u001b[32m      ]\u001b[0m    16            0.100029            0.148847                3.76\n",
      "\u001b[32m[ \u001b[0m  17%\u001b[32m      ]\u001b[0m    17            0.099974            0.148777                3.77\n",
      "\u001b[32m[ \u001b[0m  18%\u001b[32m      ]\u001b[0m    18            0.099923            0.148744                3.80\n",
      "\u001b[32m[ \u001b[0m  19%\u001b[32m      ]\u001b[0m    19            0.099893            0.148608                3.77\n",
      "\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m    20            0.099848            0.148552                3.79\n",
      "\u001b[32m[ \u001b[0m  21%\u001b[32m      ]\u001b[0m    21            0.099800            0.148514                3.78\n",
      "\u001b[32m[ \u001b[0m  22%\u001b[32m      ]\u001b[0m    22            0.099766            0.148495                3.76\n",
      "\u001b[32m[ \u001b[0m  23%\u001b[32m      ]\u001b[0m    23            0.099743            0.148424                3.79\n",
      "\u001b[32m[ \u001b[0m  24%\u001b[32m      ]\u001b[0m    24            0.099717            0.148389                3.81\n",
      "\u001b[32m[ \u001b[0m  25%\u001b[32m      ]\u001b[0m    25            0.099676            0.148346                3.78\n",
      "\u001b[32m[ \u001b[0m  26%\u001b[32m      ]\u001b[0m    26            0.099641            0.148316                3.82\n",
      "\u001b[32m[ \u001b[0m  27%\u001b[32m      ]\u001b[0m    27            0.099618            0.148279                3.77\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    28            0.099594            0.148185                3.76\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    29            0.099555            0.148111                3.78\n",
      "\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m    30            0.099545            0.148071                3.76\n",
      "\u001b[32m[ \u001b[0m  31%\u001b[32m      ]\u001b[0m    31            0.099511            0.148027                3.78\n",
      "\u001b[32m[ \u001b[0m  32%\u001b[32m      ]\u001b[0m    32            0.099489            0.148003                3.78\n",
      "\u001b[32m[ \u001b[0m  33%\u001b[32m      ]\u001b[0m    33            0.099457            0.147933                3.78\n",
      "\u001b[32m[ \u001b[0m  34%\u001b[32m      ]\u001b[0m    34            0.099461            0.147841                3.78\n",
      "\u001b[32m[ \u001b[0m  35%\u001b[32m      ]\u001b[0m    35            0.099446            0.147778                3.76\n",
      "\u001b[32m[ \u001b[0m  36%\u001b[32m      ]\u001b[0m    36            0.099429            0.147795                3.77\n",
      "\u001b[32m[ \u001b[0m  37%\u001b[32m      ]\u001b[0m    37            0.099409            0.147739                3.78\n",
      "\u001b[32m[ \u001b[0m  38%\u001b[32m      ]\u001b[0m    38            0.099391            0.147739                3.76\n",
      "\u001b[32m[ \u001b[0m  39%\u001b[32m      ]\u001b[0m    39            0.099385            0.147732                3.80\n",
      "\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m    40            0.099390            0.147627                3.77\n",
      "\u001b[32m[ \u001b[0m  41%\u001b[32m      ]\u001b[0m    41            0.099367            0.147633                3.77\n",
      "\u001b[32m[ \u001b[0m  42%\u001b[32m      ]\u001b[0m    42            0.099358            0.147611                3.76\n",
      "\u001b[32m[ \u001b[0m  43%\u001b[32m      ]\u001b[0m    43            0.099348            0.147582                3.76\n",
      "\u001b[32m[ \u001b[0m  44%\u001b[32m      ]\u001b[0m    44            0.099341            0.147553                3.77\n",
      "\u001b[32m[ \u001b[0m  45%\u001b[32m      ]\u001b[0m    45            0.099364            0.147537                3.76\n",
      "\u001b[32m[ \u001b[0m  46%\u001b[32m      ]\u001b[0m    46            0.099364            0.147490                3.82\n",
      "\u001b[32m[ \u001b[0m  47%\u001b[32m      ]\u001b[0m    47            0.099344            0.147470                3.77\n",
      "\u001b[32m[ \u001b[0m  48%\u001b[32m      ]\u001b[0m    48            0.099346            0.147433                3.77\n",
      "\u001b[32m[ \u001b[0m  49%\u001b[32m      ]\u001b[0m    49            0.099327            0.147403                3.78\n",
      "\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m    50            0.099340            0.147404                3.78\n",
      "\u001b[32m[ \u001b[0m  51%\u001b[32m      ]\u001b[0m    51            0.099342            0.147389                3.77\n",
      "\u001b[32m[ \u001b[0m  52%\u001b[32m      ]\u001b[0m    52            0.099338            0.147391                3.87\n",
      "\u001b[32m[ \u001b[0m  53%\u001b[32m      ]\u001b[0m    53            0.099337            0.147362                3.76\n",
      "\u001b[32m[ \u001b[0m  54%\u001b[32m      ]\u001b[0m    54            0.099332            0.147324                3.79\n",
      "\u001b[32m[ \u001b[0m  55%\u001b[32m      ]\u001b[0m    55            0.099338            0.147296                3.78\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    56            0.099347            0.147302                3.78\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    57            0.099347            0.147308                3.77\n",
      "\u001b[32m[ \u001b[0m  57%\u001b[32m      ]\u001b[0m    58            0.099335            0.147329                3.77\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Early-stopping at epoch 55, best loss: 0.147296\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel file: ./model_k=4_lr=0.01544019997510863_lambda=0.0009792717121814608.out\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 227.93 (sec)\u001b[0m\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=4_lr=0.01544019997510863_lambda=0.0009792717121814608.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 4\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.23 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.147295\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.47 (sec)\u001b[0m\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.1473  \u001b[0m | \u001b[0m 2.113   \u001b[0m | \u001b[0m-3.009   \u001b[0m | \u001b[0m-1.811   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for training task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/train.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of Field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 8.53 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel size: 14.26 MB\n",
      "\u001b[32m[------------] \u001b[0mTime cost for model initial: 0.01 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n",
      "\u001b[32m[------------]\u001b[0m Epoch      Train log_loss       Test log_loss     Time cost (sec)\n",
      "\u001b[32m[ \u001b[0m   1%\u001b[32m      ]\u001b[0m     1            0.104197            0.149500                6.17\n",
      "\u001b[32m[ \u001b[0m   2%\u001b[32m      ]\u001b[0m     2            0.100476            0.147466                6.11\n",
      "\u001b[32m[ \u001b[0m   3%\u001b[32m      ]\u001b[0m     3            0.099761            0.146588                6.24\n",
      "\u001b[32m[ \u001b[0m   4%\u001b[32m      ]\u001b[0m     4            0.099381            0.146089                6.14\n",
      "\u001b[32m[ \u001b[0m   5%\u001b[32m      ]\u001b[0m     5            0.099099            0.145756                6.15\n",
      "\u001b[32m[ \u001b[0m   6%\u001b[32m      ]\u001b[0m     6            0.098898            0.145420                6.12\n",
      "\u001b[32m[ \u001b[0m   7%\u001b[32m      ]\u001b[0m     7            0.098703            0.145219                6.14\n",
      "\u001b[32m[ \u001b[0m   8%\u001b[32m      ]\u001b[0m     8            0.098574            0.145058                6.11\n",
      "\u001b[32m[ \u001b[0m   9%\u001b[32m      ]\u001b[0m     9            0.098453            0.144964                6.22\n",
      "\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m    10            0.098360            0.144768                6.21\n",
      "\u001b[32m[ \u001b[0m  11%\u001b[32m      ]\u001b[0m    11            0.098280            0.144636                6.16\n",
      "\u001b[32m[ \u001b[0m  12%\u001b[32m      ]\u001b[0m    12            0.098208            0.144608                6.13\n",
      "\u001b[32m[ \u001b[0m  13%\u001b[32m      ]\u001b[0m    13            0.098138            0.144531                6.15\n",
      "\u001b[32m[ \u001b[0m  14%\u001b[32m      ]\u001b[0m    14            0.098078            0.144471                6.16\n",
      "\u001b[32m[ \u001b[0m  15%\u001b[32m      ]\u001b[0m    15            0.098032            0.144412                6.14\n",
      "\u001b[32m[ \u001b[0m  16%\u001b[32m      ]\u001b[0m    16            0.097983            0.144348                6.14\n",
      "\u001b[32m[ \u001b[0m  17%\u001b[32m      ]\u001b[0m    17            0.097948            0.144325                6.17\n",
      "\u001b[32m[ \u001b[0m  18%\u001b[32m      ]\u001b[0m    18            0.097905            0.144312                6.16\n",
      "\u001b[32m[ \u001b[0m  19%\u001b[32m      ]\u001b[0m    19            0.097875            0.144224                6.14\n",
      "\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m    20            0.097841            0.144226                6.15\n",
      "\u001b[32m[ \u001b[0m  21%\u001b[32m      ]\u001b[0m    21            0.097780            0.144307                6.16\n",
      "\u001b[32m[ \u001b[0m  22%\u001b[32m      ]\u001b[0m    22            0.097713            0.144205                6.14\n",
      "\u001b[32m[ \u001b[0m  23%\u001b[32m      ]\u001b[0m    23            0.097642            0.144161                6.13\n",
      "\u001b[32m[ \u001b[0m  24%\u001b[32m      ]\u001b[0m    24            0.097607            0.144117                6.15\n",
      "\u001b[32m[ \u001b[0m  25%\u001b[32m      ]\u001b[0m    25            0.097557            0.144039                6.15\n",
      "\u001b[32m[ \u001b[0m  26%\u001b[32m      ]\u001b[0m    26            0.097512            0.144022                6.15\n",
      "\u001b[32m[ \u001b[0m  27%\u001b[32m      ]\u001b[0m    27            0.097465            0.143946                6.16\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    28            0.097425            0.143879                6.16\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    29            0.097401            0.143912                6.16\n",
      "\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m    30            0.097351            0.143844                6.15\n",
      "\u001b[32m[ \u001b[0m  31%\u001b[32m      ]\u001b[0m    31            0.097312            0.143806                6.16\n",
      "\u001b[32m[ \u001b[0m  32%\u001b[32m      ]\u001b[0m    32            0.097286            0.143825                6.14\n",
      "\u001b[32m[ \u001b[0m  33%\u001b[32m      ]\u001b[0m    33            0.097250            0.143743                6.14\n",
      "\u001b[32m[ \u001b[0m  34%\u001b[32m      ]\u001b[0m    34            0.097213            0.143712                6.17\n",
      "\u001b[32m[ \u001b[0m  35%\u001b[32m      ]\u001b[0m    35            0.097203            0.143658                6.14\n",
      "\u001b[32m[ \u001b[0m  36%\u001b[32m      ]\u001b[0m    36            0.097162            0.143647                6.15\n",
      "\u001b[32m[ \u001b[0m  37%\u001b[32m      ]\u001b[0m    37            0.097114            0.143617                6.15\n",
      "\u001b[32m[ \u001b[0m  38%\u001b[32m      ]\u001b[0m    38            0.097091            0.143594                6.18\n",
      "\u001b[32m[ \u001b[0m  39%\u001b[32m      ]\u001b[0m    39            0.097080            0.143587                6.21\n",
      "\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m    40            0.097035            0.143551                6.13\n",
      "\u001b[32m[ \u001b[0m  41%\u001b[32m      ]\u001b[0m    41            0.097006            0.143523                6.12\n",
      "\u001b[32m[ \u001b[0m  42%\u001b[32m      ]\u001b[0m    42            0.096999            0.143513                6.15\n",
      "\u001b[32m[ \u001b[0m  43%\u001b[32m      ]\u001b[0m    43            0.096977            0.143491                6.15\n",
      "\u001b[32m[ \u001b[0m  44%\u001b[32m      ]\u001b[0m    44            0.096955            0.143487                6.12\n",
      "\u001b[32m[ \u001b[0m  45%\u001b[32m      ]\u001b[0m    45            0.096931            0.143452                6.15\n",
      "\u001b[32m[ \u001b[0m  46%\u001b[32m      ]\u001b[0m    46            0.096918            0.143387                6.17\n",
      "\u001b[32m[ \u001b[0m  47%\u001b[32m      ]\u001b[0m    47            0.096901            0.143404                6.11\n",
      "\u001b[32m[ \u001b[0m  48%\u001b[32m      ]\u001b[0m    48            0.096876            0.143446                6.16\n",
      "\u001b[32m[ \u001b[0m  49%\u001b[32m      ]\u001b[0m    49            0.096837            0.143406                6.16\n",
      "\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m    50            0.096811            0.143391                6.15\n",
      "\u001b[32m[ \u001b[0m  51%\u001b[32m      ]\u001b[0m    51            0.096790            0.143362                6.15\n",
      "\u001b[32m[ \u001b[0m  52%\u001b[32m      ]\u001b[0m    52            0.096765            0.143327                6.08\n",
      "\u001b[32m[ \u001b[0m  53%\u001b[32m      ]\u001b[0m    53            0.096739            0.143285                6.17\n",
      "\u001b[32m[ \u001b[0m  54%\u001b[32m      ]\u001b[0m    54            0.096717            0.143291                6.15\n",
      "\u001b[32m[ \u001b[0m  55%\u001b[32m      ]\u001b[0m    55            0.096700            0.143253                6.15\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    56            0.096679            0.143237                6.13\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    57            0.096654            0.143202                6.15\n",
      "\u001b[32m[ \u001b[0m  57%\u001b[32m      ]\u001b[0m    58            0.096631            0.143181                6.15\n",
      "\u001b[32m[ \u001b[0m  59%\u001b[32m      ]\u001b[0m    59            0.096614            0.143182                6.19\n",
      "\u001b[32m[ \u001b[0m  60%\u001b[32m      ]\u001b[0m    60            0.096598            0.143153                6.12\n",
      "\u001b[32m[ \u001b[0m  61%\u001b[32m      ]\u001b[0m    61            0.096578            0.143160                6.17\n",
      "\u001b[32m[ \u001b[0m  62%\u001b[32m      ]\u001b[0m    62            0.096561            0.143129                6.16\n",
      "\u001b[32m[ \u001b[0m  63%\u001b[32m      ]\u001b[0m    63            0.096534            0.143133                6.17\n",
      "\u001b[32m[ \u001b[0m  64%\u001b[32m      ]\u001b[0m    64            0.096516            0.143140                6.13\n",
      "\u001b[32m[ \u001b[0m  65%\u001b[32m      ]\u001b[0m    65            0.096495            0.143100                6.14\n",
      "\u001b[32m[ \u001b[0m  66%\u001b[32m      ]\u001b[0m    66            0.096479            0.143078                6.17\n",
      "\u001b[32m[ \u001b[0m  67%\u001b[32m      ]\u001b[0m    67            0.096459            0.143057                6.14\n",
      "\u001b[32m[ \u001b[0m  68%\u001b[32m      ]\u001b[0m    68            0.096438            0.143064                6.16\n",
      "\u001b[32m[ \u001b[0m  69%\u001b[32m      ]\u001b[0m    69            0.096420            0.143047                6.14\n",
      "\u001b[32m[ \u001b[0m  70%\u001b[32m      ]\u001b[0m    70            0.096398            0.143050                6.14\n",
      "\u001b[32m[ \u001b[0m  71%\u001b[32m      ]\u001b[0m    71            0.096384            0.142992                6.16\n",
      "\u001b[32m[ \u001b[0m  72%\u001b[32m      ]\u001b[0m    72            0.096368            0.142976                6.16\n",
      "\u001b[32m[ \u001b[0m  73%\u001b[32m      ]\u001b[0m    73            0.096343            0.142978                6.13\n",
      "\u001b[32m[ \u001b[0m  74%\u001b[32m      ]\u001b[0m    74            0.096329            0.142957                6.17\n",
      "\u001b[32m[ \u001b[0m  75%\u001b[32m      ]\u001b[0m    75            0.096310            0.142936                6.16\n",
      "\u001b[32m[ \u001b[0m  76%\u001b[32m      ]\u001b[0m    76            0.096302            0.142915                6.16\n",
      "\u001b[32m[ \u001b[0m  77%\u001b[32m      ]\u001b[0m    77            0.096287            0.142893                6.17\n",
      "\u001b[32m[ \u001b[0m  78%\u001b[32m      ]\u001b[0m    78            0.096275            0.142847                6.66\n",
      "\u001b[32m[ \u001b[0m  79%\u001b[32m      ]\u001b[0m    79            0.096265            0.142886                6.47\n",
      "\u001b[32m[ \u001b[0m  80%\u001b[32m      ]\u001b[0m    80            0.096254            0.142874                6.33\n",
      "\u001b[32m[ \u001b[0m  81%\u001b[32m      ]\u001b[0m    81            0.096237            0.142865                6.11\n",
      "\u001b[32m[ \u001b[0m  82%\u001b[32m      ]\u001b[0m    82            0.096227            0.142865                6.35\n",
      "\u001b[32m[ \u001b[0m  83%\u001b[32m      ]\u001b[0m    83            0.096210            0.142846                6.23\n",
      "\u001b[32m[ \u001b[0m  84%\u001b[32m      ]\u001b[0m    84            0.096205            0.142857                6.17\n",
      "\u001b[32m[ \u001b[0m  85%\u001b[32m      ]\u001b[0m    85            0.096195            0.142818                6.14\n",
      "\u001b[32m[ \u001b[0m  86%\u001b[32m      ]\u001b[0m    86            0.096185            0.142784                6.14\n",
      "\u001b[32m[ \u001b[0m  87%\u001b[32m      ]\u001b[0m    87            0.096177            0.142782                6.15\n",
      "\u001b[32m[ \u001b[0m  88%\u001b[32m      ]\u001b[0m    88            0.096157            0.142763                6.15\n",
      "\u001b[32m[ \u001b[0m  89%\u001b[32m      ]\u001b[0m    89            0.096153            0.142746                6.13\n",
      "\u001b[32m[ \u001b[0m  90%\u001b[32m      ]\u001b[0m    90            0.096137            0.142749                6.14\n",
      "\u001b[32m[ \u001b[0m  91%\u001b[32m      ]\u001b[0m    91            0.096127            0.142762                6.12\n",
      "\u001b[32m[ \u001b[0m  92%\u001b[32m      ]\u001b[0m    92            0.096114            0.142760                6.18\n",
      "\u001b[32m[ \u001b[0m  93%\u001b[32m      ]\u001b[0m    93            0.096111            0.142758                6.09\n",
      "\u001b[32m[ \u001b[0m  94%\u001b[32m      ]\u001b[0m    94            0.096095            0.142714                6.15\n",
      "\u001b[32m[ \u001b[0m  95%\u001b[32m      ]\u001b[0m    95            0.096083            0.142718                6.14\n",
      "\u001b[32m[ \u001b[0m  96%\u001b[32m      ]\u001b[0m    96            0.096076            0.142725                6.16\n",
      "\u001b[32m[ \u001b[0m  97%\u001b[32m      ]\u001b[0m    97            0.096066            0.142711                6.15\n",
      "\u001b[32m[ \u001b[0m  98%\u001b[32m      ]\u001b[0m    98            0.096060            0.142699                6.15\n",
      "\u001b[32m[ \u001b[0m  99%\u001b[32m      ]\u001b[0m    99            0.096053            0.142704                6.14\n",
      "\u001b[32m[ \u001b[0m 100%\u001b[32m      ]\u001b[0m   100            0.096053            0.142720                6.16\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Early-stopping at epoch 98, best loss: 0.142699\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel file: ./model_k=32_lr=0.004999999999999999_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving model: 0.02 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 625.00 (sec)\u001b[0m\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=32_lr=0.004999999999999999_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 32\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.30 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.142699\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.55 (sec)\u001b[0m\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.1427  \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m-4.699   \u001b[0m | \u001b[0m-2.301   \u001b[0m |\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for training task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/train.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val1.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of Field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 8.45 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Initialize model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel size: 5.40 MB\n",
      "\u001b[32m[------------] \u001b[0mTime cost for model initial: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to train ...\u001b[0m\n",
      "\u001b[32m[------------]\u001b[0m Epoch      Train log_loss       Test log_loss     Time cost (sec)\n",
      "\u001b[32m[ \u001b[0m   1%\u001b[32m      ]\u001b[0m     1            0.104983            0.150636                4.85\n",
      "\u001b[32m[ \u001b[0m   2%\u001b[32m      ]\u001b[0m     2            0.101074            0.148729                4.81\n",
      "\u001b[32m[ \u001b[0m   3%\u001b[32m      ]\u001b[0m     3            0.100342            0.147748                4.82\n",
      "\u001b[32m[ \u001b[0m   4%\u001b[32m      ]\u001b[0m     4            0.099935            0.147191                4.85\n",
      "\u001b[32m[ \u001b[0m   5%\u001b[32m      ]\u001b[0m     5            0.099639            0.146738                4.81\n",
      "\u001b[32m[ \u001b[0m   6%\u001b[32m      ]\u001b[0m     6            0.099444            0.146255                4.84\n",
      "\u001b[32m[ \u001b[0m   7%\u001b[32m      ]\u001b[0m     7            0.099263            0.146017                4.81\n",
      "\u001b[32m[ \u001b[0m   8%\u001b[32m      ]\u001b[0m     8            0.099112            0.145740                4.88\n",
      "\u001b[32m[ \u001b[0m   9%\u001b[32m      ]\u001b[0m     9            0.098971            0.145552                4.85\n",
      "\u001b[32m[ \u001b[0m  10%\u001b[32m      ]\u001b[0m    10            0.098871            0.145427                4.82\n",
      "\u001b[32m[ \u001b[0m  11%\u001b[32m      ]\u001b[0m    11            0.098753            0.145259                4.83\n",
      "\u001b[32m[ \u001b[0m  12%\u001b[32m      ]\u001b[0m    12            0.098678            0.145145                4.84\n",
      "\u001b[32m[ \u001b[0m  13%\u001b[32m      ]\u001b[0m    13            0.098618            0.145006                4.85\n",
      "\u001b[32m[ \u001b[0m  14%\u001b[32m      ]\u001b[0m    14            0.098561            0.144919                4.83\n",
      "\u001b[32m[ \u001b[0m  15%\u001b[32m      ]\u001b[0m    15            0.098494            0.144805                4.83\n",
      "\u001b[32m[ \u001b[0m  16%\u001b[32m      ]\u001b[0m    16            0.098454            0.144752                4.86\n",
      "\u001b[32m[ \u001b[0m  17%\u001b[32m      ]\u001b[0m    17            0.098391            0.144646                4.91\n",
      "\u001b[32m[ \u001b[0m  18%\u001b[32m      ]\u001b[0m    18            0.098351            0.144595                4.90\n",
      "\u001b[32m[ \u001b[0m  19%\u001b[32m      ]\u001b[0m    19            0.098308            0.144534                4.80\n",
      "\u001b[32m[ \u001b[0m  20%\u001b[32m      ]\u001b[0m    20            0.098290            0.144515                4.81\n",
      "\u001b[32m[ \u001b[0m  21%\u001b[32m      ]\u001b[0m    21            0.098247            0.144484                4.82\n",
      "\u001b[32m[ \u001b[0m  22%\u001b[32m      ]\u001b[0m    22            0.098219            0.144448                4.80\n",
      "\u001b[32m[ \u001b[0m  23%\u001b[32m      ]\u001b[0m    23            0.098188            0.144408                4.81\n",
      "\u001b[32m[ \u001b[0m  24%\u001b[32m      ]\u001b[0m    24            0.098160            0.144388                4.82\n",
      "\u001b[32m[ \u001b[0m  25%\u001b[32m      ]\u001b[0m    25            0.098141            0.144385                4.82\n",
      "\u001b[32m[ \u001b[0m  26%\u001b[32m      ]\u001b[0m    26            0.098122            0.144378                4.80\n",
      "\u001b[32m[ \u001b[0m  27%\u001b[32m      ]\u001b[0m    27            0.098086            0.144421                4.82\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    28            0.098011            0.144354                4.82\n",
      "\u001b[32m[ \u001b[0m  28%\u001b[32m      ]\u001b[0m    29            0.097969            0.144299                4.78\n",
      "\u001b[32m[ \u001b[0m  30%\u001b[32m      ]\u001b[0m    30            0.097918            0.144210                4.85\n",
      "\u001b[32m[ \u001b[0m  31%\u001b[32m      ]\u001b[0m    31            0.097872            0.144183                4.84\n",
      "\u001b[32m[ \u001b[0m  32%\u001b[32m      ]\u001b[0m    32            0.097846            0.144125                4.83\n",
      "\u001b[32m[ \u001b[0m  33%\u001b[32m      ]\u001b[0m    33            0.097799            0.144103                4.86\n",
      "\u001b[32m[ \u001b[0m  34%\u001b[32m      ]\u001b[0m    34            0.097761            0.144055                4.83\n",
      "\u001b[32m[ \u001b[0m  35%\u001b[32m      ]\u001b[0m    35            0.097729            0.144040                4.81\n",
      "\u001b[32m[ \u001b[0m  36%\u001b[32m      ]\u001b[0m    36            0.097682            0.144017                4.83\n",
      "\u001b[32m[ \u001b[0m  37%\u001b[32m      ]\u001b[0m    37            0.097664            0.143999                4.83\n",
      "\u001b[32m[ \u001b[0m  38%\u001b[32m      ]\u001b[0m    38            0.097623            0.143944                4.84\n",
      "\u001b[32m[ \u001b[0m  39%\u001b[32m      ]\u001b[0m    39            0.097605            0.143957                4.84\n",
      "\u001b[32m[ \u001b[0m  40%\u001b[32m      ]\u001b[0m    40            0.097582            0.143950                4.83\n",
      "\u001b[32m[ \u001b[0m  41%\u001b[32m      ]\u001b[0m    41            0.097544            0.143946                4.83\n",
      "\u001b[32m[ \u001b[0m  42%\u001b[32m      ]\u001b[0m    42            0.097512            0.143910                4.82\n",
      "\u001b[32m[ \u001b[0m  43%\u001b[32m      ]\u001b[0m    43            0.097486            0.143876                4.83\n",
      "\u001b[32m[ \u001b[0m  44%\u001b[32m      ]\u001b[0m    44            0.097465            0.143895                4.83\n",
      "\u001b[32m[ \u001b[0m  45%\u001b[32m      ]\u001b[0m    45            0.097442            0.143883                4.82\n",
      "\u001b[32m[ \u001b[0m  46%\u001b[32m      ]\u001b[0m    46            0.097414            0.143841                4.82\n",
      "\u001b[32m[ \u001b[0m  47%\u001b[32m      ]\u001b[0m    47            0.097390            0.143810                4.82\n",
      "\u001b[32m[ \u001b[0m  48%\u001b[32m      ]\u001b[0m    48            0.097361            0.143808                4.82\n",
      "\u001b[32m[ \u001b[0m  49%\u001b[32m      ]\u001b[0m    49            0.097334            0.143786                4.80\n",
      "\u001b[32m[ \u001b[0m  50%\u001b[32m      ]\u001b[0m    50            0.097327            0.143788                4.81\n",
      "\u001b[32m[ \u001b[0m  51%\u001b[32m      ]\u001b[0m    51            0.097305            0.143778                4.81\n",
      "\u001b[32m[ \u001b[0m  52%\u001b[32m      ]\u001b[0m    52            0.097278            0.143770                4.83\n",
      "\u001b[32m[ \u001b[0m  53%\u001b[32m      ]\u001b[0m    53            0.097247            0.143736                4.83\n",
      "\u001b[32m[ \u001b[0m  54%\u001b[32m      ]\u001b[0m    54            0.097234            0.143723                4.83\n",
      "\u001b[32m[ \u001b[0m  55%\u001b[32m      ]\u001b[0m    55            0.097217            0.143696                4.82\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    56            0.097207            0.143688                4.83\n",
      "\u001b[32m[ \u001b[0m  56%\u001b[32m      ]\u001b[0m    57            0.097199            0.143664                4.83\n",
      "\u001b[32m[ \u001b[0m  57%\u001b[32m      ]\u001b[0m    58            0.097180            0.143643                4.83\n",
      "\u001b[32m[ \u001b[0m  59%\u001b[32m      ]\u001b[0m    59            0.097169            0.143650                4.82\n",
      "\u001b[32m[ \u001b[0m  60%\u001b[32m      ]\u001b[0m    60            0.097147            0.143632                4.84\n",
      "\u001b[32m[ \u001b[0m  61%\u001b[32m      ]\u001b[0m    61            0.097138            0.143608                4.84\n",
      "\u001b[32m[ \u001b[0m  62%\u001b[32m      ]\u001b[0m    62            0.097126            0.143603                4.82\n",
      "\u001b[32m[ \u001b[0m  63%\u001b[32m      ]\u001b[0m    63            0.097110            0.143586                4.84\n",
      "\u001b[32m[ \u001b[0m  64%\u001b[32m      ]\u001b[0m    64            0.097099            0.143580                4.82\n",
      "\u001b[32m[ \u001b[0m  65%\u001b[32m      ]\u001b[0m    65            0.097083            0.143570                4.82\n",
      "\u001b[32m[ \u001b[0m  66%\u001b[32m      ]\u001b[0m    66            0.097086            0.143561                4.83\n",
      "\u001b[32m[ \u001b[0m  67%\u001b[32m      ]\u001b[0m    67            0.097071            0.143563                4.84\n",
      "\u001b[32m[ \u001b[0m  68%\u001b[32m      ]\u001b[0m    68            0.097042            0.143609                4.87\n",
      "\u001b[32m[ \u001b[0m  69%\u001b[32m      ]\u001b[0m    69            0.097015            0.143588                4.83\n",
      "\u001b[32m[ \u001b[0m  70%\u001b[32m      ]\u001b[0m    70            0.096987            0.143564                4.84\n",
      "\u001b[32m[ \u001b[0m  71%\u001b[32m      ]\u001b[0m    71            0.096987            0.143548                4.84\n",
      "\u001b[32m[ \u001b[0m  72%\u001b[32m      ]\u001b[0m    72            0.096970            0.143534                4.84\n",
      "\u001b[32m[ \u001b[0m  73%\u001b[32m      ]\u001b[0m    73            0.096943            0.143513                4.83\n",
      "\u001b[32m[ \u001b[0m  74%\u001b[32m      ]\u001b[0m    74            0.096931            0.143491                4.85\n",
      "\u001b[32m[ \u001b[0m  75%\u001b[32m      ]\u001b[0m    75            0.096921            0.143469                4.84\n",
      "\u001b[32m[ \u001b[0m  76%\u001b[32m      ]\u001b[0m    76            0.096904            0.143472                4.83\n",
      "\u001b[32m[ \u001b[0m  77%\u001b[32m      ]\u001b[0m    77            0.096894            0.143438                4.88\n",
      "\u001b[32m[ \u001b[0m  78%\u001b[32m      ]\u001b[0m    78            0.096872            0.143413                4.80\n",
      "\u001b[32m[ \u001b[0m  79%\u001b[32m      ]\u001b[0m    79            0.096874            0.143412                4.99\n",
      "\u001b[32m[ \u001b[0m  80%\u001b[32m      ]\u001b[0m    80            0.096869            0.143396                4.89\n",
      "\u001b[32m[ \u001b[0m  81%\u001b[32m      ]\u001b[0m    81            0.096867            0.143377                4.82\n",
      "\u001b[32m[ \u001b[0m  82%\u001b[32m      ]\u001b[0m    82            0.096861            0.143360                4.82\n",
      "\u001b[32m[ \u001b[0m  83%\u001b[32m      ]\u001b[0m    83            0.096853            0.143345                4.85\n",
      "\u001b[32m[ \u001b[0m  84%\u001b[32m      ]\u001b[0m    84            0.096857            0.143349                4.83\n",
      "\u001b[32m[ \u001b[0m  85%\u001b[32m      ]\u001b[0m    85            0.096825            0.143348                4.83\n",
      "\u001b[32m[ \u001b[0m  86%\u001b[32m      ]\u001b[0m    86            0.096816            0.143330                4.83\n",
      "\u001b[32m[ \u001b[0m  87%\u001b[32m      ]\u001b[0m    87            0.096810            0.143331                4.84\n",
      "\u001b[32m[ \u001b[0m  88%\u001b[32m      ]\u001b[0m    88            0.096798            0.143327                4.84\n",
      "\u001b[32m[ \u001b[0m  89%\u001b[32m      ]\u001b[0m    89            0.096781            0.143343                4.85\n",
      "\u001b[32m[ \u001b[0m  90%\u001b[32m      ]\u001b[0m    90            0.096772            0.143313                4.85\n",
      "\u001b[32m[ \u001b[0m  91%\u001b[32m      ]\u001b[0m    91            0.096753            0.143297                4.85\n",
      "\u001b[32m[ \u001b[0m  92%\u001b[32m      ]\u001b[0m    92            0.096750            0.143301                4.83\n",
      "\u001b[32m[ \u001b[0m  93%\u001b[32m      ]\u001b[0m    93            0.096745            0.143300                4.82\n",
      "\u001b[32m[ \u001b[0m  94%\u001b[32m      ]\u001b[0m    94            0.096736            0.143287                4.84\n",
      "\u001b[32m[ \u001b[0m  95%\u001b[32m      ]\u001b[0m    95            0.096728            0.143281                4.85\n",
      "\u001b[32m[ \u001b[0m  96%\u001b[32m      ]\u001b[0m    96            0.096722            0.143266                4.80\n",
      "\u001b[32m[ \u001b[0m  97%\u001b[32m      ]\u001b[0m    97            0.096710            0.143268                4.82\n",
      "\u001b[32m[ \u001b[0m  98%\u001b[32m      ]\u001b[0m    98            0.096698            0.143249                4.82\n",
      "\u001b[32m[ \u001b[0m  99%\u001b[32m      ]\u001b[0m    99            0.096696            0.143232                4.82\n",
      "\u001b[32m[ \u001b[0m 100%\u001b[32m      ]\u001b[0m   100            0.096684            0.143209                4.83\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to save model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mModel file: ./model_k=12_lr=0.004999999999999999_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mTime cost for saving model: 0.01 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Finish training\u001b[0m\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 491.83 (sec)\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.miniforge3_x64/envs/recsys/lib/python3.8/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (3.694503293355064, -4.698970004336019, -2.3010299956639813)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_x/6b2126bj7mn3gqzwmrrhj79w0000gn/T/ipykernel_50123/3891148978.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/_x/6b2126bj7mn3gqzwmrrhj79w0000gn/T/ipykernel_50123/3884868368.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, n_iter)\u001b[0m\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         self.bayesopt.maximize(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniforge3_x64/envs/recsys/lib/python3.8/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniforge3_x64/envs/recsys/lib/python3.8/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniforge3_x64/envs/recsys/lib/python3.8/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_x/6b2126bj7mn3gqzwmrrhj79w0000gn/T/ipykernel_50123/3884868368.py\u001b[0m in \u001b[0;36mblackbox\u001b[0;34m(self, k_, lr_, l2_coeff_)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblackbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_coeff_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mk_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mlr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0ml2_coeff_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_coeff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/_x/6b2126bj7mn3gqzwmrrhj79w0000gn/T/ipykernel_50123/3884868368.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, k, lr, l2_coeff)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mffm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniforge3_x64/envs/recsys/lib/python3.8/site-packages/xlearn/xlearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, param, model_path)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_Param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLearnFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.start(n_iter=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "502b0d96-d16a-4e7a-8570-47acb5561050",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=8_lr=0.20000000000000004_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 8\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.27 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.163332\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.52 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 8, 'lr': 0.20000000000000004, 'epoch': 100, 'lambda': 2e-05} has logloss=0.16334645013279583 on val2 set\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=8_lr=0.20000000000000004_lambda=0.00020000000000000004.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 8\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.24 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.165072\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.49 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 8, 'lr': 0.20000000000000004, 'epoch': 100, 'lambda': 0.00020000000000000004} has logloss=0.16508409157229761 on val2 set\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=4_lr=0.20000000000000004_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 4\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.25 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.164133\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.49 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 4, 'lr': 0.20000000000000004, 'epoch': 100, 'lambda': 2e-05} has logloss=0.16413969589318922 on val2 set\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=13_lr=0.018239710656162565_lambda=0.001682711340723979.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 13\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.28 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.172612\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.53 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 13, 'lr': 0.018239710656162565, 'epoch': 100, 'lambda': 0.001682711340723979} has logloss=0.1726100717473283 on val2 set\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=10_lr=0.013633150644635009_lambda=9.098523214437794e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 10\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.27 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.164552\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.52 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 10, 'lr': 0.013633150644635009, 'epoch': 100, 'lambda': 9.098523214437794e-05} has logloss=0.1645503974223327 on val2 set\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=6_lr=0.019829923480712375_lambda=3.637481691522213e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 6\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.26 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.164495\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.50 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 6, 'lr': 0.019829923480712375, 'epoch': 100, 'lambda': 3.637481691522213e-05} has logloss=0.16449520368905146 on val2 set\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=9_lr=0.007500353216451047_lambda=0.00012527174573451596.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 9\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.25 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.165783\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.50 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 9, 'lr': 0.007500353216451047, 'epoch': 100, 'lambda': 0.00012527174573451596} has logloss=0.16577420812984592 on val2 set\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=4_lr=0.01544019997510863_lambda=0.0009792717121814608.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 4\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.26 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.169837\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.51 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 4, 'lr': 0.01544019997510863, 'epoch': 100, 'lambda': 0.0009792717121814608} has logloss=0.16983777209976977 on val2 set\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=32_lr=0.004999999999999999_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 32\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.01 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/val2.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.26 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.164517\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 0.52 (sec)\u001b[0m\n",
      "{'task': 'binary', 'k': 32, 'lr': 0.004999999999999999, 'epoch': 100, 'lambda': 2e-05} has logloss=0.16452298784053201 on val2 set\n",
      "we take {'task': 'binary', 'k': 8, 'lr': 0.20000000000000004, 'epoch': 100, 'lambda': 2e-05} with logloss=0.1633 as the best model\n",
      "\u001b[32m\u001b[1m----------------------------------------------------------------------------------------------\n",
      "           _\n",
      "          | |\n",
      "     __  _| |     ___  __ _ _ __ _ __\n",
      "     \\ \\/ / |    / _ \\/ _` | '__| '_ \\ \n",
      "      >  <| |___|  __/ (_| | |  | | | |\n",
      "     /_/\\_\\_____/\\___|\\__,_|_|  |_| |_|\n",
      "\n",
      "        xLearn   -- 0.40 Version --\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[39m\u001b[0m\u001b[32m[------------] \u001b[0mxLearn uses 8 threads for prediction task.\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Load model ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mLoad model from ./model_k=8_lr=0.20000000000000004_lambda=2e-05.out\n",
      "\u001b[32m[------------] \u001b[0mLoss function: cross-entropy\n",
      "\u001b[32m[------------] \u001b[0mScore function: ffm\n",
      "\u001b[32m[------------] \u001b[0mNumber of Feature: 11606\n",
      "\u001b[32m[------------] \u001b[0mNumber of K: 8\n",
      "\u001b[32m[------------] \u001b[0mNumber of field: 5\n",
      "\u001b[32m[------------] \u001b[0mTime cost for loading model: 0.00 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Read Problem ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mFirst check if the text file has been already converted to binary format.\n",
      "\u001b[32m[------------] \u001b[0mBinary file (../data/lastday.txt.bin) found. Skip converting text to binary.\n",
      "\u001b[32m[------------] \u001b[0mTime cost for reading problem: 0.85 (sec)\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Start to predict ...\u001b[0m\n",
      "\u001b[32m[------------] \u001b[0mThe test loss is: 0.144722\n",
      "\u001b[32m\u001b[1m[ ACTION     ] Clear the xLearn environment ...\u001b[0m\n",
      "\u001b[32m\u001b[1m[------------] Total time cost: 1.61 (sec)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# не дождался всех 24 шагов\n",
    "\n",
    "# выбираем лучшую модель по val2\n",
    "models = trainer.models\n",
    "params = trainer.params\n",
    "\n",
    "best_model = None\n",
    "best_score = 10e6\n",
    "best_param = None\n",
    "for model_path, param in zip(models, params):\n",
    "    score = validate(model_path, \"../data/val2.txt\")\n",
    "    print(f\"{param} has logloss={score} on val2 set\")\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_model = model_path\n",
    "        best_param = param\n",
    "\n",
    "print(f\"we take {best_param} with logloss={best_score:.4f} as the best model\")\n",
    "\n",
    "# применяем лучшую модель на тестовых данных (последний день)\n",
    "test_score = validate(best_model, test_set=\"../data/lastday.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46966f21-8df4-4eda-93a3-777dd869e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best model has logloss=0.1447 on last day data\n"
     ]
    }
   ],
   "source": [
    "print(f\"the best model has logloss={test_score:.4f} on last day data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c55ace1-be48-490b-975a-8fe6cf1bcc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# у модели из первого дз логлосс на последнем дне 0.1378\n",
    "\n",
    "# слишком разный логлосс на трейне и тесте - переобучаемся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09f4fc-bdbf-4c20-ae2c-7b6c2442a425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
